{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjWvnaQUrZmD"
      },
      "source": [
        "# Emotion classification using the RAVDESS dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldtHMhuLrewK"
      },
      "source": [
        "The RAVDESS contains 7356 files. Each file was rated 10 times on emotional validity, intensity, and genuineness. Ratings were provided by 247 individuals who were characteristic of untrained adult research participants from North America. A further set of 72 participants provided test-retest data. High levels of emotional validity, interrater reliability, and test-retest intrarater reliability were reported. Validation data is open-access, and can be downloaded along with our paper from PLOS ONE.\n",
        "\n",
        "***Description***\n",
        "\n",
        "The dataset contains the complete set of 7356 RAVDESS files (total size: 24.8 GB). Each of the 24 actors consists of three modality formats: Audio-only (16bit, 48kHz .wav), Audio-Video (720p H.264, AAC 48kHz, .mp4), and Video-only (no sound).  Note, there are no song files for Actor_18.\n",
        "\n",
        "***Data***\n",
        "\n",
        "For this task, I have used 4948 samples from the RAVDESS dataset.\n",
        "\n",
        "The samples comes from:\n",
        "\n",
        "- Audio-only files;\n",
        "- Video + audio files: I have extracted the audio from each file using the script Mp4ToWav.py that you can find in the main directory of the project.\n",
        "\n",
        "***License information***\n",
        "\n",
        "The RAVDESS is released under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License, CC BY-NA-SC 4.0\n",
        "\n",
        "***File naming convention***\n",
        "\n",
        "Each of the 7356 RAVDESS files has a unique filename. The filename consists of a 7-part numerical identifier (e.g., 02-01-06-01-02-01-12.mp4). These identifiers define the stimulus characteristics:\n",
        "\n",
        "***Filename identifiers***\n",
        "\n",
        "- Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n",
        "- Vocal channel (01 = speech, 02 = song).\n",
        "- Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n",
        "- Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the ‘neutral’ emotion.\n",
        "- Statement (01 = “Kids are talking by the door”, 02 = “Dogs are sitting by the door”).\n",
        "- Repetition (01 = 1st repetition, 02 = 2nd repetition).\n",
        "- Actor (01 to 24. Odd numbered actors are male, even numbered actors are female).\n",
        "\n",
        "Filename example: 02-01-06-01-02-01-12.mp4 \n",
        "\n",
        "- Video-only (02)\n",
        "- Speech (01)\n",
        "- Fearful (06)\n",
        "- Normal intensity (01)\n",
        "- Statement “dogs” (02)\n",
        "- 1st Repetition (01)\n",
        "- 12th Actor (12)\n",
        "- Female, as the actor ID number is even."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDNbxj45rkvB"
      },
      "source": [
        "# Analysis\n",
        "\n",
        "We are using Colab, a Google Cloud environment for jupyter, so we need to import our files from Google Drive and then install LibROSA, a python package for music and audio analysis.\n",
        "\n",
        "After the import, we will plot the signal of the first file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-o2JI49WBAe",
        "outputId": "a66573aa-04fe-48a4-ad61-76d4efd18816"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxI4xzngdS-e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "outputId": "5ba1be0e-863d-4991-98e4-f5a48cf8b482"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/librosa/core/audio.py:165: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/librosa/core/audio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0msf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSoundFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msf_desc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m             \u001b[0msr_native\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msf_desc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplerate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/soundfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd)\u001b[0m\n\u001b[1;32m    628\u001b[0m                                          format, subtype, endian)\n\u001b[0;32m--> 629\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosefd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missuperset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'r+'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseekable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/soundfile.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[1;32m   1183\u001b[0m         _error_check(_snd.sf_error(file_ptr),\n\u001b[0;32m-> 1184\u001b[0;31m                      \"Error opening {0!r}: \".format(self.name))\n\u001b[0m\u001b[1;32m   1185\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode_int\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_snd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSFM_WRITE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/soundfile.py\u001b[0m in \u001b[0;36m_error_check\u001b[0;34m(err, prefix)\u001b[0m\n\u001b[1;32m   1356\u001b[0m         \u001b[0merr_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_snd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msf_error_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1357\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0m_ffi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'replace'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error opening '/content/drive/MyDrive/Datasets/Audio_Speech_Actors_01-24/Actor_01/03-01-01-01-01-01-01.wav': System error.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-9afae55da72f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlibrosa\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampling_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Datasets/Audio_Speech_Actors_01-24/Actor_01/03-01-01-01-01-01-01.wav'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/librosa/core/audio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPurePath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PySoundFile failed. Trying audioread instead.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr_native\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__audioread_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mduration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/librosa/core/audio.py\u001b[0m in \u001b[0;36m__audioread_load\u001b[0;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0maudioread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minput_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0msr_native\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplerate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mn_channels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchannels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/audioread/__init__.py\u001b[0m in \u001b[0;36maudio_open\u001b[0;34m(path, backends)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mBackendClass\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbackends\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mBackendClass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mDecodeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/audioread/rawread.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \"\"\"\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Datasets/Audio_Speech_Actors_01-24/Actor_01/03-01-01-01-01-01-01.wav'"
          ]
        }
      ],
      "source": [
        "import librosa\n",
        "from librosa import display\n",
        "\n",
        "data, sampling_rate = librosa.load('/content/drive/MyDrive/Datasets/Audio_Speech_Actors_01-24/Actor_01/03-01-01-01-01-01-01.wav')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "id": "WgaSHtCIdtX2",
        "outputId": "6ba02e06-4f15-4a6a-fe16-5f494b27fb53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['display']\n",
            "`%matplotlib` prevents importing * from pylab and numpy\n",
            "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PolyCollection at 0x7f4690165090>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "% pylab inline\n",
        "import os\n",
        "import pandas as pd\n",
        "import glob \n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "librosa.display.waveplot(data, sr=sampling_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCtNuVWlr5jL"
      },
      "source": [
        "# Load all files\n",
        "\n",
        "We will create our numpy array extracting Mel-frequency cepstral coefficients (MFCCs), while the classes to predict will be extracted from the name of the file (see the introductory section of this notebook to see the naming convention of the files of this dataset)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKvuF--gd6F-",
        "outputId": "38644e0a-e619-4027-bbf6-d005eede45cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Data loaded. Loading time: 361.5156126022339 seconds ---\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "path = '/content/drive/MyDrive/Colab Notebooks/emotion-classification-from-audio-files/features'\n",
        "lst = []\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for subdir, dirs, files in os.walk(path):\n",
        "  for file in files:\n",
        "      try:\n",
        "        #Load librosa array, obtain mfcss, store the file and the mcss information in a new array\n",
        "        X, sample_rate = librosa.load(os.path.join(subdir,file), res_type='kaiser_fast')\n",
        "        mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0) \n",
        "        # The instruction below converts the labels (from 1 to 8) to a series from 0 to 7\n",
        "        # This is because our predictor needs to start from 0 otherwise it will try to predict also 0.\n",
        "        file = int(file[7:8]) - 1 \n",
        "        arr = mfccs, file\n",
        "        lst.append(arr)\n",
        "      # If the file is not valid, skip it\n",
        "      except ValueError:\n",
        "        continue\n",
        "\n",
        "print(\"--- Data loaded. Loading time: %s seconds ---\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLSggnF7kKY1"
      },
      "outputs": [],
      "source": [
        "# Creating X and y: zip makes a list of all the first elements, and a list of all the second elements.\n",
        "X, y = zip(*lst)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VzvBRTJIlIE9",
        "outputId": "fa2567b5-a5c0-40cd-e360-c967c48e7a59"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((5252, 40), (5252,))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "import numpy as np\n",
        "X = np.asarray(X)\n",
        "y = np.asarray(y)\n",
        "\n",
        "\n",
        "X.shape, y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOutQiAlCjOY"
      },
      "outputs": [],
      "source": [
        "# Saving joblib files to not load them again with the loop above\n",
        "\n",
        "import joblib\n",
        "\n",
        "X_name = 'X.joblib'\n",
        "y_name = 'y.joblib'\n",
        "save_dir = '/content/drive/MyDrive/Colab Notebooks/emotion-classification-from-audio-files/our_test_files'\n",
        "\n",
        "savedX = joblib.dump(X, os.path.join(save_dir, X_name))\n",
        "savedy = joblib.dump(y, os.path.join(save_dir, y_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIoFdycUXMxA"
      },
      "outputs": [],
      "source": [
        "# Loading saved models\n",
        "import joblib\n",
        "X = joblib.load('/content/drive/MyDrive/Colab Notebooks/emotion-classification-from-audio-files/joblib_features/X.joblib')\n",
        "y = joblib.load('/content/drive/MyDrive/Colab Notebooks/emotion-classification-from-audio-files/joblib_features/y.joblib')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nnRtWaqCeFi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Agw-3KN1sDhh"
      },
      "source": [
        "# Decision Tree Classifier\n",
        "\n",
        "To make a first attempt in accomplishing this classification task I chose a decision tree:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-Xgb5NslTBO"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UshLOC1ClWL3"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BnCR52nlXw0"
      },
      "outputs": [],
      "source": [
        "dtree = DecisionTreeClassifier()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWyTownblZM0",
        "outputId": "0ed75d84-7268-475c-a190-f27b3f10c185"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier()"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "dtree.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEuw6TUQlr7C"
      },
      "outputs": [],
      "source": [
        "predictions = dtree.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LvhLc_40OF7",
        "outputId": "e62dac0e-b89f-467b-fefe-23061cea6cb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7, 1, 1, ..., 3, 5, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1v0i0V7sMw7"
      },
      "source": [
        "Let's go with our classification report.\n",
        "\n",
        "Before we start, a quick reminder of the classes we are trying to predict:\n",
        "\n",
        "emotions = {\n",
        "    \"neutral\": \"0\",\n",
        "    \"calm\": \"1\",\n",
        "    \"happy\": \"2\",\n",
        "    \"sad\": \"3\",\n",
        "    \"angry\": \"4\", \n",
        "    \"fearful\": \"5\", \n",
        "    \"disgust\": \"6\", \n",
        "    \"surprised\": \"7\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4kNSYkAleIv",
        "outputId": "2e8efb08-9a2c-47d5-9834-b818699f9880"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.78      0.76       192\n",
            "           1       0.60      0.52      0.56       123\n",
            "           2       0.64      0.63      0.63       264\n",
            "           3       0.71      0.66      0.68       275\n",
            "           4       0.70      0.69      0.70       252\n",
            "           5       0.59      0.66      0.62       241\n",
            "           6       0.68      0.69      0.69       197\n",
            "           7       0.59      0.61      0.60       190\n",
            "\n",
            "    accuracy                           0.66      1734\n",
            "   macro avg       0.66      0.65      0.66      1734\n",
            "weighted avg       0.66      0.66      0.66      1734\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "print(classification_report(y_test,predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCVgjLj-gwE2"
      },
      "source": [
        "# Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfaTxzZ1w__y"
      },
      "source": [
        "In this second approach, I switched to a random forest classifier and I made a gridsearch to make some hyperparameters tuning.\n",
        "\n",
        "The gridsearch is not shown in the code below otherwise the notebook will require too much time to run."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9eqMHV3S8i6"
      },
      "source": [
        "# Neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-QscoyMxQtn"
      },
      "source": [
        "Let's build our neural network!\n",
        "\n",
        "To do so, we need to expand the dimensions of our array, adding a third one using the numpy \"expand_dims\" feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4i187-Pe-w5"
      },
      "outputs": [],
      "source": [
        "x_traincnn = np.expand_dims(X_train, axis=2)\n",
        "x_testcnn = np.expand_dims(X_test, axis=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnvoCRX1gQCh",
        "outputId": "9b58b8f9-1816-4d4c-c539-f76d211f3b47"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((3518, 40, 1), (1734, 40, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "x_traincnn.shape, x_testcnn.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZOGIpuefCd3",
        "outputId": "6236fda8-7818-4d43-e535-fcc1b821ba67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.layers import Input, Flatten, Dropout, Activation\n",
        "from keras.layers import Conv1D, MaxPooling1D\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import RMSprop \n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv1D(128, 5,padding='same',\n",
        "                 input_shape=(40,1)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(MaxPooling1D(pool_size=(8)))\n",
        "model.add(Conv1D(128, 5,padding='same',))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(8))\n",
        "model.add(Activation('softmax'))\n",
        "opt = RMSprop(lr=0.00005, rho=0.9, epsilon=None, decay=0.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LphftMIZzUvz"
      },
      "source": [
        "With *model.summary* we can see a recap of what we have build:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIWPB4Zgfic7",
        "outputId": "9252f402-6cf4-4191-ceeb-63246cb5c9e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_10 (Conv1D)          (None, 40, 128)           768       \n",
            "                                                                 \n",
            " activation_15 (Activation)  (None, 40, 128)           0         \n",
            "                                                                 \n",
            " dropout_10 (Dropout)        (None, 40, 128)           0         \n",
            "                                                                 \n",
            " max_pooling1d_5 (MaxPooling  (None, 5, 128)           0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " conv1d_11 (Conv1D)          (None, 5, 128)            82048     \n",
            "                                                                 \n",
            " activation_16 (Activation)  (None, 5, 128)            0         \n",
            "                                                                 \n",
            " dropout_11 (Dropout)        (None, 5, 128)            0         \n",
            "                                                                 \n",
            " flatten_5 (Flatten)         (None, 640)               0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 8)                 5128      \n",
            "                                                                 \n",
            " activation_17 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 87,944\n",
            "Trainable params: 87,944\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qQSBeBhzcLu"
      },
      "source": [
        "Now we can compile and fit our model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNI1znbsfpTx"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktdF-nJKfq6F",
        "outputId": "7c69244b-f8c3-47b6-ca03-e7fe2fa4cb66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "220/220 [==============================] - 13s 6ms/step - loss: 6.0139 - accuracy: 0.1777 - val_loss: 2.0117 - val_accuracy: 0.3472\n",
            "Epoch 2/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 3.9672 - accuracy: 0.2706 - val_loss: 1.5860 - val_accuracy: 0.4319\n",
            "Epoch 3/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 2.9413 - accuracy: 0.3425 - val_loss: 1.4691 - val_accuracy: 0.5121\n",
            "Epoch 4/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 2.3235 - accuracy: 0.3906 - val_loss: 1.3003 - val_accuracy: 0.5306\n",
            "Epoch 5/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 1.8610 - accuracy: 0.4471 - val_loss: 1.2530 - val_accuracy: 0.5692\n",
            "Epoch 6/1000\n",
            "220/220 [==============================] - 1s 4ms/step - loss: 1.6005 - accuracy: 0.4920 - val_loss: 1.2808 - val_accuracy: 0.5640\n",
            "Epoch 7/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 1.5038 - accuracy: 0.5048 - val_loss: 1.1890 - val_accuracy: 0.6228\n",
            "Epoch 8/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 1.3655 - accuracy: 0.5341 - val_loss: 1.1576 - val_accuracy: 0.6200\n",
            "Epoch 9/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 1.3281 - accuracy: 0.5520 - val_loss: 1.0960 - val_accuracy: 0.6217\n",
            "Epoch 10/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 1.2615 - accuracy: 0.5713 - val_loss: 1.1074 - val_accuracy: 0.6367\n",
            "Epoch 11/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 1.2042 - accuracy: 0.5964 - val_loss: 1.0327 - val_accuracy: 0.6338\n",
            "Epoch 12/1000\n",
            "220/220 [==============================] - 1s 4ms/step - loss: 1.1652 - accuracy: 0.5890 - val_loss: 1.0233 - val_accuracy: 0.6286\n",
            "Epoch 13/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 1.1241 - accuracy: 0.6171 - val_loss: 1.0022 - val_accuracy: 0.6459\n",
            "Epoch 14/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 1.0823 - accuracy: 0.6254 - val_loss: 0.9760 - val_accuracy: 0.6690\n",
            "Epoch 15/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 1.0656 - accuracy: 0.6239 - val_loss: 0.9388 - val_accuracy: 0.6638\n",
            "Epoch 16/1000\n",
            "220/220 [==============================] - 1s 4ms/step - loss: 1.0262 - accuracy: 0.6379 - val_loss: 0.9493 - val_accuracy: 0.6505\n",
            "Epoch 17/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 1.0318 - accuracy: 0.6353 - val_loss: 0.9107 - val_accuracy: 0.6667\n",
            "Epoch 18/1000\n",
            "220/220 [==============================] - 1s 4ms/step - loss: 0.9896 - accuracy: 0.6475 - val_loss: 0.8977 - val_accuracy: 0.6794\n",
            "Epoch 19/1000\n",
            "220/220 [==============================] - 1s 4ms/step - loss: 0.9754 - accuracy: 0.6595 - val_loss: 0.9020 - val_accuracy: 0.6765\n",
            "Epoch 20/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.9531 - accuracy: 0.6569 - val_loss: 0.8661 - val_accuracy: 0.6938\n",
            "Epoch 21/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.9360 - accuracy: 0.6649 - val_loss: 0.8824 - val_accuracy: 0.6886\n",
            "Epoch 22/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.9385 - accuracy: 0.6640 - val_loss: 0.8536 - val_accuracy: 0.6788\n",
            "Epoch 23/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.9043 - accuracy: 0.6748 - val_loss: 0.8375 - val_accuracy: 0.6845\n",
            "Epoch 24/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.9022 - accuracy: 0.6768 - val_loss: 0.8515 - val_accuracy: 0.6736\n",
            "Epoch 25/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.8955 - accuracy: 0.6802 - val_loss: 0.8293 - val_accuracy: 0.6915\n",
            "Epoch 26/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.8864 - accuracy: 0.6879 - val_loss: 0.8235 - val_accuracy: 0.6880\n",
            "Epoch 27/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.8651 - accuracy: 0.6856 - val_loss: 0.7987 - val_accuracy: 0.7134\n",
            "Epoch 28/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.8555 - accuracy: 0.6944 - val_loss: 0.8126 - val_accuracy: 0.6880\n",
            "Epoch 29/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.8451 - accuracy: 0.6924 - val_loss: 0.8103 - val_accuracy: 0.6869\n",
            "Epoch 30/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.8273 - accuracy: 0.6970 - val_loss: 0.7797 - val_accuracy: 0.7168\n",
            "Epoch 31/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.8203 - accuracy: 0.6976 - val_loss: 0.7912 - val_accuracy: 0.7024\n",
            "Epoch 32/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.8244 - accuracy: 0.6947 - val_loss: 0.7741 - val_accuracy: 0.7174\n",
            "Epoch 33/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.8070 - accuracy: 0.7049 - val_loss: 0.7621 - val_accuracy: 0.7111\n",
            "Epoch 34/1000\n",
            "220/220 [==============================] - 2s 7ms/step - loss: 0.8107 - accuracy: 0.6978 - val_loss: 0.7788 - val_accuracy: 0.6978\n",
            "Epoch 35/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.7948 - accuracy: 0.7049 - val_loss: 0.7575 - val_accuracy: 0.7157\n",
            "Epoch 36/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.7827 - accuracy: 0.7129 - val_loss: 0.7395 - val_accuracy: 0.7255\n",
            "Epoch 37/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.7661 - accuracy: 0.7172 - val_loss: 0.7694 - val_accuracy: 0.7042\n",
            "Epoch 38/1000\n",
            "220/220 [==============================] - 1s 4ms/step - loss: 0.7636 - accuracy: 0.7274 - val_loss: 0.7449 - val_accuracy: 0.7249\n",
            "Epoch 39/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.7565 - accuracy: 0.7274 - val_loss: 0.7682 - val_accuracy: 0.6995\n",
            "Epoch 40/1000\n",
            "220/220 [==============================] - 1s 4ms/step - loss: 0.7538 - accuracy: 0.7223 - val_loss: 0.7309 - val_accuracy: 0.7278\n",
            "Epoch 41/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.7536 - accuracy: 0.7209 - val_loss: 0.7292 - val_accuracy: 0.7290\n",
            "Epoch 42/1000\n",
            "220/220 [==============================] - 1s 4ms/step - loss: 0.7434 - accuracy: 0.7229 - val_loss: 0.7118 - val_accuracy: 0.7353\n",
            "Epoch 43/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.7354 - accuracy: 0.7334 - val_loss: 0.7082 - val_accuracy: 0.7382\n",
            "Epoch 44/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.7263 - accuracy: 0.7331 - val_loss: 0.7156 - val_accuracy: 0.7295\n",
            "Epoch 45/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.7273 - accuracy: 0.7277 - val_loss: 0.6964 - val_accuracy: 0.7468\n",
            "Epoch 46/1000\n",
            "220/220 [==============================] - 1s 4ms/step - loss: 0.7072 - accuracy: 0.7413 - val_loss: 0.7036 - val_accuracy: 0.7290\n",
            "Epoch 47/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.7113 - accuracy: 0.7345 - val_loss: 0.6941 - val_accuracy: 0.7451\n",
            "Epoch 48/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.7159 - accuracy: 0.7308 - val_loss: 0.7183 - val_accuracy: 0.7451\n",
            "Epoch 49/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.7110 - accuracy: 0.7436 - val_loss: 0.6904 - val_accuracy: 0.7537\n",
            "Epoch 50/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.7020 - accuracy: 0.7456 - val_loss: 0.6918 - val_accuracy: 0.7503\n",
            "Epoch 51/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.6881 - accuracy: 0.7493 - val_loss: 0.7145 - val_accuracy: 0.7393\n",
            "Epoch 52/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.6822 - accuracy: 0.7536 - val_loss: 0.7037 - val_accuracy: 0.7520\n",
            "Epoch 53/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.6862 - accuracy: 0.7442 - val_loss: 0.6890 - val_accuracy: 0.7428\n",
            "Epoch 54/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.6788 - accuracy: 0.7430 - val_loss: 0.6879 - val_accuracy: 0.7313\n",
            "Epoch 55/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.6764 - accuracy: 0.7538 - val_loss: 0.6914 - val_accuracy: 0.7341\n",
            "Epoch 56/1000\n",
            "220/220 [==============================] - 1s 4ms/step - loss: 0.6658 - accuracy: 0.7570 - val_loss: 0.6840 - val_accuracy: 0.7422\n",
            "Epoch 57/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.6663 - accuracy: 0.7646 - val_loss: 0.6719 - val_accuracy: 0.7509\n",
            "Epoch 58/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.6595 - accuracy: 0.7618 - val_loss: 0.6575 - val_accuracy: 0.7543\n",
            "Epoch 59/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.6565 - accuracy: 0.7555 - val_loss: 0.6690 - val_accuracy: 0.7526\n",
            "Epoch 60/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.6558 - accuracy: 0.7626 - val_loss: 0.6540 - val_accuracy: 0.7526\n",
            "Epoch 61/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.6492 - accuracy: 0.7609 - val_loss: 0.6540 - val_accuracy: 0.7566\n",
            "Epoch 62/1000\n",
            "220/220 [==============================] - 1s 4ms/step - loss: 0.6398 - accuracy: 0.7661 - val_loss: 0.6564 - val_accuracy: 0.7584\n",
            "Epoch 63/1000\n",
            "220/220 [==============================] - 1s 4ms/step - loss: 0.6492 - accuracy: 0.7624 - val_loss: 0.6548 - val_accuracy: 0.7555\n",
            "Epoch 64/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.6422 - accuracy: 0.7678 - val_loss: 0.6811 - val_accuracy: 0.7428\n",
            "Epoch 65/1000\n",
            "220/220 [==============================] - 1s 4ms/step - loss: 0.6427 - accuracy: 0.7615 - val_loss: 0.6430 - val_accuracy: 0.7601\n",
            "Epoch 66/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.6324 - accuracy: 0.7663 - val_loss: 0.6485 - val_accuracy: 0.7636\n",
            "Epoch 67/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.6274 - accuracy: 0.7692 - val_loss: 0.6530 - val_accuracy: 0.7566\n",
            "Epoch 68/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.6264 - accuracy: 0.7723 - val_loss: 0.6375 - val_accuracy: 0.7630\n",
            "Epoch 69/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.6342 - accuracy: 0.7698 - val_loss: 0.6638 - val_accuracy: 0.7572\n",
            "Epoch 70/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.6296 - accuracy: 0.7678 - val_loss: 0.6442 - val_accuracy: 0.7578\n",
            "Epoch 71/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.6182 - accuracy: 0.7752 - val_loss: 0.6582 - val_accuracy: 0.7561\n",
            "Epoch 72/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.6074 - accuracy: 0.7803 - val_loss: 0.6466 - val_accuracy: 0.7532\n",
            "Epoch 73/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.6119 - accuracy: 0.7746 - val_loss: 0.6493 - val_accuracy: 0.7653\n",
            "Epoch 74/1000\n",
            "220/220 [==============================] - 1s 4ms/step - loss: 0.6110 - accuracy: 0.7763 - val_loss: 0.6487 - val_accuracy: 0.7595\n",
            "Epoch 75/1000\n",
            "220/220 [==============================] - 1s 4ms/step - loss: 0.6136 - accuracy: 0.7754 - val_loss: 0.6315 - val_accuracy: 0.7664\n",
            "Epoch 76/1000\n",
            "220/220 [==============================] - 1s 4ms/step - loss: 0.5991 - accuracy: 0.7840 - val_loss: 0.6302 - val_accuracy: 0.7589\n",
            "Epoch 77/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.5984 - accuracy: 0.7823 - val_loss: 0.6142 - val_accuracy: 0.7745\n",
            "Epoch 78/1000\n",
            "220/220 [==============================] - 2s 8ms/step - loss: 0.5987 - accuracy: 0.7757 - val_loss: 0.6280 - val_accuracy: 0.7745\n",
            "Epoch 79/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.5870 - accuracy: 0.7857 - val_loss: 0.6154 - val_accuracy: 0.7739\n",
            "Epoch 80/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.5966 - accuracy: 0.7814 - val_loss: 0.6188 - val_accuracy: 0.7745\n",
            "Epoch 81/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.5851 - accuracy: 0.7834 - val_loss: 0.6421 - val_accuracy: 0.7636\n",
            "Epoch 82/1000\n",
            "220/220 [==============================] - 1s 4ms/step - loss: 0.5909 - accuracy: 0.7837 - val_loss: 0.6409 - val_accuracy: 0.7630\n",
            "Epoch 83/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.5777 - accuracy: 0.7902 - val_loss: 0.6123 - val_accuracy: 0.7716\n",
            "Epoch 84/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.5902 - accuracy: 0.7885 - val_loss: 0.6185 - val_accuracy: 0.7676\n",
            "Epoch 85/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.5807 - accuracy: 0.7894 - val_loss: 0.6262 - val_accuracy: 0.7618\n",
            "Epoch 86/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.5845 - accuracy: 0.7882 - val_loss: 0.6012 - val_accuracy: 0.7849\n",
            "Epoch 87/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.5715 - accuracy: 0.7891 - val_loss: 0.6042 - val_accuracy: 0.7768\n",
            "Epoch 88/1000\n",
            "220/220 [==============================] - 2s 8ms/step - loss: 0.5745 - accuracy: 0.7905 - val_loss: 0.6063 - val_accuracy: 0.7791\n",
            "Epoch 89/1000\n",
            "220/220 [==============================] - 1s 7ms/step - loss: 0.5735 - accuracy: 0.7959 - val_loss: 0.6089 - val_accuracy: 0.7768\n",
            "Epoch 90/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.5550 - accuracy: 0.7959 - val_loss: 0.5938 - val_accuracy: 0.7826\n",
            "Epoch 91/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.5595 - accuracy: 0.7942 - val_loss: 0.5916 - val_accuracy: 0.7780\n",
            "Epoch 92/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.5491 - accuracy: 0.7976 - val_loss: 0.6088 - val_accuracy: 0.7797\n",
            "Epoch 93/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.5659 - accuracy: 0.7948 - val_loss: 0.5956 - val_accuracy: 0.7837\n",
            "Epoch 94/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.5583 - accuracy: 0.7936 - val_loss: 0.5920 - val_accuracy: 0.7785\n",
            "Epoch 95/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.5549 - accuracy: 0.7914 - val_loss: 0.5900 - val_accuracy: 0.7820\n",
            "Epoch 96/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.5536 - accuracy: 0.8024 - val_loss: 0.5937 - val_accuracy: 0.7791\n",
            "Epoch 97/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.5498 - accuracy: 0.7973 - val_loss: 0.6023 - val_accuracy: 0.7860\n",
            "Epoch 98/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.5435 - accuracy: 0.8027 - val_loss: 0.6009 - val_accuracy: 0.7757\n",
            "Epoch 99/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.5405 - accuracy: 0.8107 - val_loss: 0.5783 - val_accuracy: 0.7826\n",
            "Epoch 100/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.5458 - accuracy: 0.7996 - val_loss: 0.5998 - val_accuracy: 0.7739\n",
            "Epoch 101/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.5420 - accuracy: 0.8050 - val_loss: 0.5945 - val_accuracy: 0.7797\n",
            "Epoch 102/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.5245 - accuracy: 0.8096 - val_loss: 0.5978 - val_accuracy: 0.7809\n",
            "Epoch 103/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.5440 - accuracy: 0.8013 - val_loss: 0.5788 - val_accuracy: 0.7884\n",
            "Epoch 104/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.5416 - accuracy: 0.8050 - val_loss: 0.6052 - val_accuracy: 0.7722\n",
            "Epoch 105/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.5356 - accuracy: 0.8022 - val_loss: 0.5881 - val_accuracy: 0.7797\n",
            "Epoch 106/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.5339 - accuracy: 0.8044 - val_loss: 0.5999 - val_accuracy: 0.7791\n",
            "Epoch 107/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.5262 - accuracy: 0.8070 - val_loss: 0.5724 - val_accuracy: 0.7860\n",
            "Epoch 108/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.5289 - accuracy: 0.8141 - val_loss: 0.5808 - val_accuracy: 0.7837\n",
            "Epoch 109/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.5242 - accuracy: 0.8084 - val_loss: 0.5731 - val_accuracy: 0.7924\n",
            "Epoch 110/1000\n",
            "220/220 [==============================] - 1s 4ms/step - loss: 0.5307 - accuracy: 0.8087 - val_loss: 0.5784 - val_accuracy: 0.7953\n",
            "Epoch 111/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.5177 - accuracy: 0.8115 - val_loss: 0.5652 - val_accuracy: 0.7907\n",
            "Epoch 112/1000\n",
            "220/220 [==============================] - 1s 4ms/step - loss: 0.5172 - accuracy: 0.8115 - val_loss: 0.5769 - val_accuracy: 0.7930\n",
            "Epoch 113/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.5214 - accuracy: 0.8124 - val_loss: 0.5739 - val_accuracy: 0.7918\n",
            "Epoch 114/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.5118 - accuracy: 0.8084 - val_loss: 0.5626 - val_accuracy: 0.7889\n",
            "Epoch 115/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.5180 - accuracy: 0.8138 - val_loss: 0.5689 - val_accuracy: 0.7866\n",
            "Epoch 116/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.5142 - accuracy: 0.8115 - val_loss: 0.5686 - val_accuracy: 0.7843\n",
            "Epoch 117/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.5095 - accuracy: 0.8124 - val_loss: 0.5682 - val_accuracy: 0.7999\n",
            "Epoch 118/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.5106 - accuracy: 0.8130 - val_loss: 0.5683 - val_accuracy: 0.7924\n",
            "Epoch 119/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.5133 - accuracy: 0.8189 - val_loss: 0.5631 - val_accuracy: 0.7958\n",
            "Epoch 120/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.5068 - accuracy: 0.8132 - val_loss: 0.5589 - val_accuracy: 0.7907\n",
            "Epoch 121/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.5034 - accuracy: 0.8138 - val_loss: 0.5687 - val_accuracy: 0.7826\n",
            "Epoch 122/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.5058 - accuracy: 0.8172 - val_loss: 0.5654 - val_accuracy: 0.7907\n",
            "Epoch 123/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4971 - accuracy: 0.8189 - val_loss: 0.5725 - val_accuracy: 0.7820\n",
            "Epoch 124/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4957 - accuracy: 0.8150 - val_loss: 0.5550 - val_accuracy: 0.7924\n",
            "Epoch 125/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.5038 - accuracy: 0.8147 - val_loss: 0.5725 - val_accuracy: 0.7832\n",
            "Epoch 126/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4827 - accuracy: 0.8260 - val_loss: 0.5724 - val_accuracy: 0.7953\n",
            "Epoch 127/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4976 - accuracy: 0.8189 - val_loss: 0.5538 - val_accuracy: 0.7947\n",
            "Epoch 128/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4880 - accuracy: 0.8240 - val_loss: 0.5733 - val_accuracy: 0.7780\n",
            "Epoch 129/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4896 - accuracy: 0.8249 - val_loss: 0.5480 - val_accuracy: 0.7953\n",
            "Epoch 130/1000\n",
            "220/220 [==============================] - 1s 4ms/step - loss: 0.4926 - accuracy: 0.8138 - val_loss: 0.5587 - val_accuracy: 0.8033\n",
            "Epoch 131/1000\n",
            "220/220 [==============================] - 1s 4ms/step - loss: 0.4865 - accuracy: 0.8221 - val_loss: 0.5490 - val_accuracy: 0.7964\n",
            "Epoch 132/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4829 - accuracy: 0.8300 - val_loss: 0.5459 - val_accuracy: 0.8051\n",
            "Epoch 133/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4876 - accuracy: 0.8206 - val_loss: 0.5626 - val_accuracy: 0.7849\n",
            "Epoch 134/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4782 - accuracy: 0.8292 - val_loss: 0.5363 - val_accuracy: 0.8103\n",
            "Epoch 135/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4836 - accuracy: 0.8258 - val_loss: 0.5707 - val_accuracy: 0.7901\n",
            "Epoch 136/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4802 - accuracy: 0.8329 - val_loss: 0.5594 - val_accuracy: 0.7970\n",
            "Epoch 137/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4699 - accuracy: 0.8371 - val_loss: 0.5522 - val_accuracy: 0.8005\n",
            "Epoch 138/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4750 - accuracy: 0.8320 - val_loss: 0.5533 - val_accuracy: 0.7918\n",
            "Epoch 139/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4777 - accuracy: 0.8297 - val_loss: 0.5467 - val_accuracy: 0.8016\n",
            "Epoch 140/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4771 - accuracy: 0.8300 - val_loss: 0.5446 - val_accuracy: 0.7947\n",
            "Epoch 141/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4790 - accuracy: 0.8275 - val_loss: 0.5415 - val_accuracy: 0.7941\n",
            "Epoch 142/1000\n",
            "220/220 [==============================] - 1s 4ms/step - loss: 0.4669 - accuracy: 0.8289 - val_loss: 0.5371 - val_accuracy: 0.8005\n",
            "Epoch 143/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4733 - accuracy: 0.8300 - val_loss: 0.5409 - val_accuracy: 0.8033\n",
            "Epoch 144/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4624 - accuracy: 0.8323 - val_loss: 0.5430 - val_accuracy: 0.7895\n",
            "Epoch 145/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4634 - accuracy: 0.8351 - val_loss: 0.5579 - val_accuracy: 0.7982\n",
            "Epoch 146/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4658 - accuracy: 0.8348 - val_loss: 0.5509 - val_accuracy: 0.7878\n",
            "Epoch 147/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4668 - accuracy: 0.8346 - val_loss: 0.5473 - val_accuracy: 0.7964\n",
            "Epoch 148/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4528 - accuracy: 0.8312 - val_loss: 0.5572 - val_accuracy: 0.8016\n",
            "Epoch 149/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4702 - accuracy: 0.8360 - val_loss: 0.5469 - val_accuracy: 0.7958\n",
            "Epoch 150/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4578 - accuracy: 0.8368 - val_loss: 0.5522 - val_accuracy: 0.8045\n",
            "Epoch 151/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4524 - accuracy: 0.8408 - val_loss: 0.5560 - val_accuracy: 0.8028\n",
            "Epoch 152/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4633 - accuracy: 0.8343 - val_loss: 0.5495 - val_accuracy: 0.7941\n",
            "Epoch 153/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4598 - accuracy: 0.8383 - val_loss: 0.5396 - val_accuracy: 0.8051\n",
            "Epoch 154/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4587 - accuracy: 0.8312 - val_loss: 0.5392 - val_accuracy: 0.8028\n",
            "Epoch 155/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4510 - accuracy: 0.8360 - val_loss: 0.5408 - val_accuracy: 0.7947\n",
            "Epoch 156/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4487 - accuracy: 0.8400 - val_loss: 0.5307 - val_accuracy: 0.8103\n",
            "Epoch 157/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4440 - accuracy: 0.8408 - val_loss: 0.5427 - val_accuracy: 0.7993\n",
            "Epoch 158/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4449 - accuracy: 0.8405 - val_loss: 0.5462 - val_accuracy: 0.7953\n",
            "Epoch 159/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4500 - accuracy: 0.8320 - val_loss: 0.5474 - val_accuracy: 0.7918\n",
            "Epoch 160/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4475 - accuracy: 0.8380 - val_loss: 0.5320 - val_accuracy: 0.8085\n",
            "Epoch 161/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4438 - accuracy: 0.8442 - val_loss: 0.5374 - val_accuracy: 0.8028\n",
            "Epoch 162/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4452 - accuracy: 0.8385 - val_loss: 0.5279 - val_accuracy: 0.8091\n",
            "Epoch 163/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4445 - accuracy: 0.8403 - val_loss: 0.5231 - val_accuracy: 0.8126\n",
            "Epoch 164/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4465 - accuracy: 0.8343 - val_loss: 0.5235 - val_accuracy: 0.8080\n",
            "Epoch 165/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4466 - accuracy: 0.8383 - val_loss: 0.5293 - val_accuracy: 0.8080\n",
            "Epoch 166/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4410 - accuracy: 0.8411 - val_loss: 0.5256 - val_accuracy: 0.8010\n",
            "Epoch 167/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4408 - accuracy: 0.8442 - val_loss: 0.5265 - val_accuracy: 0.8137\n",
            "Epoch 168/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4292 - accuracy: 0.8468 - val_loss: 0.5506 - val_accuracy: 0.8057\n",
            "Epoch 169/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4306 - accuracy: 0.8468 - val_loss: 0.5315 - val_accuracy: 0.8045\n",
            "Epoch 170/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4326 - accuracy: 0.8411 - val_loss: 0.5295 - val_accuracy: 0.8045\n",
            "Epoch 171/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4291 - accuracy: 0.8442 - val_loss: 0.5437 - val_accuracy: 0.8126\n",
            "Epoch 172/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4266 - accuracy: 0.8502 - val_loss: 0.5494 - val_accuracy: 0.7958\n",
            "Epoch 173/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4204 - accuracy: 0.8448 - val_loss: 0.5294 - val_accuracy: 0.8010\n",
            "Epoch 174/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4345 - accuracy: 0.8366 - val_loss: 0.5294 - val_accuracy: 0.8028\n",
            "Epoch 175/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4318 - accuracy: 0.8417 - val_loss: 0.5453 - val_accuracy: 0.8062\n",
            "Epoch 176/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4291 - accuracy: 0.8479 - val_loss: 0.5178 - val_accuracy: 0.8080\n",
            "Epoch 177/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4175 - accuracy: 0.8496 - val_loss: 0.5128 - val_accuracy: 0.8247\n",
            "Epoch 178/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4307 - accuracy: 0.8476 - val_loss: 0.5351 - val_accuracy: 0.7999\n",
            "Epoch 179/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4254 - accuracy: 0.8437 - val_loss: 0.5253 - val_accuracy: 0.8108\n",
            "Epoch 180/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4304 - accuracy: 0.8434 - val_loss: 0.5129 - val_accuracy: 0.8085\n",
            "Epoch 181/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4257 - accuracy: 0.8451 - val_loss: 0.5256 - val_accuracy: 0.8022\n",
            "Epoch 182/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4227 - accuracy: 0.8485 - val_loss: 0.5113 - val_accuracy: 0.8137\n",
            "Epoch 183/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4172 - accuracy: 0.8479 - val_loss: 0.5170 - val_accuracy: 0.8016\n",
            "Epoch 184/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4145 - accuracy: 0.8508 - val_loss: 0.5209 - val_accuracy: 0.8126\n",
            "Epoch 185/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4119 - accuracy: 0.8496 - val_loss: 0.5152 - val_accuracy: 0.8149\n",
            "Epoch 186/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4112 - accuracy: 0.8513 - val_loss: 0.5168 - val_accuracy: 0.8051\n",
            "Epoch 187/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4155 - accuracy: 0.8499 - val_loss: 0.5225 - val_accuracy: 0.8120\n",
            "Epoch 188/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4187 - accuracy: 0.8457 - val_loss: 0.5228 - val_accuracy: 0.8114\n",
            "Epoch 189/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4088 - accuracy: 0.8525 - val_loss: 0.5092 - val_accuracy: 0.8103\n",
            "Epoch 190/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4078 - accuracy: 0.8505 - val_loss: 0.5340 - val_accuracy: 0.8143\n",
            "Epoch 191/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4141 - accuracy: 0.8522 - val_loss: 0.5302 - val_accuracy: 0.8045\n",
            "Epoch 192/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4147 - accuracy: 0.8493 - val_loss: 0.5143 - val_accuracy: 0.8160\n",
            "Epoch 193/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4053 - accuracy: 0.8593 - val_loss: 0.5099 - val_accuracy: 0.8137\n",
            "Epoch 194/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4040 - accuracy: 0.8553 - val_loss: 0.5210 - val_accuracy: 0.7993\n",
            "Epoch 195/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4062 - accuracy: 0.8493 - val_loss: 0.5134 - val_accuracy: 0.8114\n",
            "Epoch 196/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4062 - accuracy: 0.8550 - val_loss: 0.5144 - val_accuracy: 0.8074\n",
            "Epoch 197/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4078 - accuracy: 0.8596 - val_loss: 0.5159 - val_accuracy: 0.8062\n",
            "Epoch 198/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4157 - accuracy: 0.8502 - val_loss: 0.5144 - val_accuracy: 0.8085\n",
            "Epoch 199/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3996 - accuracy: 0.8579 - val_loss: 0.4984 - val_accuracy: 0.8206\n",
            "Epoch 200/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3954 - accuracy: 0.8570 - val_loss: 0.4992 - val_accuracy: 0.8281\n",
            "Epoch 201/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3905 - accuracy: 0.8596 - val_loss: 0.5082 - val_accuracy: 0.8155\n",
            "Epoch 202/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3983 - accuracy: 0.8539 - val_loss: 0.5007 - val_accuracy: 0.8276\n",
            "Epoch 203/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.4010 - accuracy: 0.8627 - val_loss: 0.5217 - val_accuracy: 0.8091\n",
            "Epoch 204/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3924 - accuracy: 0.8601 - val_loss: 0.5106 - val_accuracy: 0.8166\n",
            "Epoch 205/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3984 - accuracy: 0.8533 - val_loss: 0.4965 - val_accuracy: 0.8126\n",
            "Epoch 206/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3905 - accuracy: 0.8636 - val_loss: 0.5140 - val_accuracy: 0.8230\n",
            "Epoch 207/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3849 - accuracy: 0.8650 - val_loss: 0.4948 - val_accuracy: 0.8155\n",
            "Epoch 208/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3889 - accuracy: 0.8599 - val_loss: 0.4966 - val_accuracy: 0.8178\n",
            "Epoch 209/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3957 - accuracy: 0.8542 - val_loss: 0.5100 - val_accuracy: 0.8149\n",
            "Epoch 210/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3797 - accuracy: 0.8681 - val_loss: 0.5190 - val_accuracy: 0.8155\n",
            "Epoch 211/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3893 - accuracy: 0.8582 - val_loss: 0.5052 - val_accuracy: 0.8126\n",
            "Epoch 212/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3927 - accuracy: 0.8607 - val_loss: 0.4993 - val_accuracy: 0.8183\n",
            "Epoch 213/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3919 - accuracy: 0.8613 - val_loss: 0.5145 - val_accuracy: 0.8085\n",
            "Epoch 214/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3849 - accuracy: 0.8621 - val_loss: 0.4973 - val_accuracy: 0.8230\n",
            "Epoch 215/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3868 - accuracy: 0.8627 - val_loss: 0.4911 - val_accuracy: 0.8131\n",
            "Epoch 216/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3814 - accuracy: 0.8661 - val_loss: 0.5010 - val_accuracy: 0.8189\n",
            "Epoch 217/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3855 - accuracy: 0.8607 - val_loss: 0.5044 - val_accuracy: 0.8224\n",
            "Epoch 218/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3796 - accuracy: 0.8627 - val_loss: 0.5055 - val_accuracy: 0.8206\n",
            "Epoch 219/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3764 - accuracy: 0.8661 - val_loss: 0.5135 - val_accuracy: 0.8074\n",
            "Epoch 220/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3756 - accuracy: 0.8630 - val_loss: 0.5073 - val_accuracy: 0.8172\n",
            "Epoch 221/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3755 - accuracy: 0.8627 - val_loss: 0.5193 - val_accuracy: 0.8016\n",
            "Epoch 222/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3781 - accuracy: 0.8621 - val_loss: 0.5075 - val_accuracy: 0.8028\n",
            "Epoch 223/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3751 - accuracy: 0.8653 - val_loss: 0.5083 - val_accuracy: 0.8149\n",
            "Epoch 224/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3814 - accuracy: 0.8678 - val_loss: 0.5011 - val_accuracy: 0.8195\n",
            "Epoch 225/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3782 - accuracy: 0.8624 - val_loss: 0.4919 - val_accuracy: 0.8183\n",
            "Epoch 226/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3735 - accuracy: 0.8681 - val_loss: 0.4926 - val_accuracy: 0.8241\n",
            "Epoch 227/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3732 - accuracy: 0.8641 - val_loss: 0.5025 - val_accuracy: 0.8131\n",
            "Epoch 228/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3764 - accuracy: 0.8658 - val_loss: 0.4946 - val_accuracy: 0.8230\n",
            "Epoch 229/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3744 - accuracy: 0.8630 - val_loss: 0.4968 - val_accuracy: 0.8166\n",
            "Epoch 230/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3671 - accuracy: 0.8709 - val_loss: 0.4950 - val_accuracy: 0.8212\n",
            "Epoch 231/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3661 - accuracy: 0.8695 - val_loss: 0.4939 - val_accuracy: 0.8212\n",
            "Epoch 232/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3714 - accuracy: 0.8684 - val_loss: 0.4927 - val_accuracy: 0.8235\n",
            "Epoch 233/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3817 - accuracy: 0.8653 - val_loss: 0.5014 - val_accuracy: 0.8195\n",
            "Epoch 234/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3638 - accuracy: 0.8718 - val_loss: 0.4959 - val_accuracy: 0.8241\n",
            "Epoch 235/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3718 - accuracy: 0.8655 - val_loss: 0.4958 - val_accuracy: 0.8281\n",
            "Epoch 236/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3639 - accuracy: 0.8741 - val_loss: 0.5050 - val_accuracy: 0.8091\n",
            "Epoch 237/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3583 - accuracy: 0.8721 - val_loss: 0.4895 - val_accuracy: 0.8189\n",
            "Epoch 238/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3649 - accuracy: 0.8690 - val_loss: 0.5178 - val_accuracy: 0.8033\n",
            "Epoch 239/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3613 - accuracy: 0.8718 - val_loss: 0.4834 - val_accuracy: 0.8230\n",
            "Epoch 240/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3621 - accuracy: 0.8695 - val_loss: 0.5066 - val_accuracy: 0.8137\n",
            "Epoch 241/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3595 - accuracy: 0.8738 - val_loss: 0.4977 - val_accuracy: 0.8189\n",
            "Epoch 242/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3635 - accuracy: 0.8732 - val_loss: 0.4981 - val_accuracy: 0.8201\n",
            "Epoch 243/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3491 - accuracy: 0.8783 - val_loss: 0.5051 - val_accuracy: 0.8137\n",
            "Epoch 244/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3586 - accuracy: 0.8704 - val_loss: 0.4943 - val_accuracy: 0.8218\n",
            "Epoch 245/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3646 - accuracy: 0.8698 - val_loss: 0.5002 - val_accuracy: 0.8183\n",
            "Epoch 246/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3488 - accuracy: 0.8778 - val_loss: 0.4983 - val_accuracy: 0.8131\n",
            "Epoch 247/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3528 - accuracy: 0.8803 - val_loss: 0.4853 - val_accuracy: 0.8212\n",
            "Epoch 248/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3529 - accuracy: 0.8744 - val_loss: 0.4817 - val_accuracy: 0.8322\n",
            "Epoch 249/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3482 - accuracy: 0.8803 - val_loss: 0.5043 - val_accuracy: 0.8189\n",
            "Epoch 250/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3466 - accuracy: 0.8778 - val_loss: 0.4937 - val_accuracy: 0.8224\n",
            "Epoch 251/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3509 - accuracy: 0.8755 - val_loss: 0.4828 - val_accuracy: 0.8241\n",
            "Epoch 252/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3484 - accuracy: 0.8715 - val_loss: 0.4940 - val_accuracy: 0.8235\n",
            "Epoch 253/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3536 - accuracy: 0.8712 - val_loss: 0.5048 - val_accuracy: 0.8114\n",
            "Epoch 254/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3479 - accuracy: 0.8758 - val_loss: 0.5012 - val_accuracy: 0.8183\n",
            "Epoch 255/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3483 - accuracy: 0.8781 - val_loss: 0.4937 - val_accuracy: 0.8160\n",
            "Epoch 256/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3483 - accuracy: 0.8764 - val_loss: 0.4825 - val_accuracy: 0.8356\n",
            "Epoch 257/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3432 - accuracy: 0.8812 - val_loss: 0.5104 - val_accuracy: 0.8131\n",
            "Epoch 258/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3424 - accuracy: 0.8758 - val_loss: 0.4900 - val_accuracy: 0.8230\n",
            "Epoch 259/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3522 - accuracy: 0.8718 - val_loss: 0.4869 - val_accuracy: 0.8166\n",
            "Epoch 260/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3530 - accuracy: 0.8764 - val_loss: 0.4926 - val_accuracy: 0.8166\n",
            "Epoch 261/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3567 - accuracy: 0.8664 - val_loss: 0.4927 - val_accuracy: 0.8201\n",
            "Epoch 262/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3425 - accuracy: 0.8778 - val_loss: 0.4705 - val_accuracy: 0.8304\n",
            "Epoch 263/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3459 - accuracy: 0.8781 - val_loss: 0.4958 - val_accuracy: 0.8166\n",
            "Epoch 264/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3428 - accuracy: 0.8758 - val_loss: 0.4812 - val_accuracy: 0.8241\n",
            "Epoch 265/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3433 - accuracy: 0.8789 - val_loss: 0.4875 - val_accuracy: 0.8212\n",
            "Epoch 266/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3428 - accuracy: 0.8727 - val_loss: 0.4785 - val_accuracy: 0.8293\n",
            "Epoch 267/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3382 - accuracy: 0.8764 - val_loss: 0.4768 - val_accuracy: 0.8253\n",
            "Epoch 268/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3450 - accuracy: 0.8744 - val_loss: 0.4917 - val_accuracy: 0.8155\n",
            "Epoch 269/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3464 - accuracy: 0.8738 - val_loss: 0.4869 - val_accuracy: 0.8201\n",
            "Epoch 270/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3453 - accuracy: 0.8795 - val_loss: 0.4835 - val_accuracy: 0.8264\n",
            "Epoch 271/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3346 - accuracy: 0.8792 - val_loss: 0.4756 - val_accuracy: 0.8270\n",
            "Epoch 272/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3374 - accuracy: 0.8812 - val_loss: 0.5016 - val_accuracy: 0.8166\n",
            "Epoch 273/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3377 - accuracy: 0.8778 - val_loss: 0.4862 - val_accuracy: 0.8322\n",
            "Epoch 274/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3314 - accuracy: 0.8835 - val_loss: 0.5070 - val_accuracy: 0.8108\n",
            "Epoch 275/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3371 - accuracy: 0.8741 - val_loss: 0.4789 - val_accuracy: 0.8212\n",
            "Epoch 276/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3222 - accuracy: 0.8823 - val_loss: 0.4870 - val_accuracy: 0.8218\n",
            "Epoch 277/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3283 - accuracy: 0.8849 - val_loss: 0.4881 - val_accuracy: 0.8137\n",
            "Epoch 278/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3250 - accuracy: 0.8872 - val_loss: 0.5017 - val_accuracy: 0.8178\n",
            "Epoch 279/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3311 - accuracy: 0.8832 - val_loss: 0.4758 - val_accuracy: 0.8304\n",
            "Epoch 280/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3326 - accuracy: 0.8783 - val_loss: 0.4866 - val_accuracy: 0.8299\n",
            "Epoch 281/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3350 - accuracy: 0.8795 - val_loss: 0.4786 - val_accuracy: 0.8264\n",
            "Epoch 282/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3213 - accuracy: 0.8880 - val_loss: 0.4801 - val_accuracy: 0.8258\n",
            "Epoch 283/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3265 - accuracy: 0.8874 - val_loss: 0.4970 - val_accuracy: 0.8189\n",
            "Epoch 284/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3349 - accuracy: 0.8792 - val_loss: 0.4832 - val_accuracy: 0.8270\n",
            "Epoch 285/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3240 - accuracy: 0.8837 - val_loss: 0.4822 - val_accuracy: 0.8201\n",
            "Epoch 286/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3186 - accuracy: 0.8877 - val_loss: 0.4627 - val_accuracy: 0.8351\n",
            "Epoch 287/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3165 - accuracy: 0.8880 - val_loss: 0.4878 - val_accuracy: 0.8379\n",
            "Epoch 288/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3261 - accuracy: 0.8852 - val_loss: 0.4775 - val_accuracy: 0.8345\n",
            "Epoch 289/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3187 - accuracy: 0.8880 - val_loss: 0.4876 - val_accuracy: 0.8258\n",
            "Epoch 290/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3159 - accuracy: 0.8874 - val_loss: 0.4637 - val_accuracy: 0.8322\n",
            "Epoch 291/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3098 - accuracy: 0.8943 - val_loss: 0.4732 - val_accuracy: 0.8356\n",
            "Epoch 292/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3207 - accuracy: 0.8891 - val_loss: 0.4833 - val_accuracy: 0.8258\n",
            "Epoch 293/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3205 - accuracy: 0.8889 - val_loss: 0.4805 - val_accuracy: 0.8247\n",
            "Epoch 294/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3255 - accuracy: 0.8866 - val_loss: 0.4769 - val_accuracy: 0.8264\n",
            "Epoch 295/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3173 - accuracy: 0.8923 - val_loss: 0.4776 - val_accuracy: 0.8166\n",
            "Epoch 296/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3101 - accuracy: 0.8857 - val_loss: 0.5053 - val_accuracy: 0.8195\n",
            "Epoch 297/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3149 - accuracy: 0.8843 - val_loss: 0.4829 - val_accuracy: 0.8293\n",
            "Epoch 298/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3165 - accuracy: 0.8872 - val_loss: 0.4953 - val_accuracy: 0.8189\n",
            "Epoch 299/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3118 - accuracy: 0.8908 - val_loss: 0.4919 - val_accuracy: 0.8195\n",
            "Epoch 300/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3128 - accuracy: 0.8872 - val_loss: 0.4884 - val_accuracy: 0.8247\n",
            "Epoch 301/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3205 - accuracy: 0.8857 - val_loss: 0.5058 - val_accuracy: 0.8195\n",
            "Epoch 302/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3015 - accuracy: 0.8897 - val_loss: 0.4807 - val_accuracy: 0.8287\n",
            "Epoch 303/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3014 - accuracy: 0.8908 - val_loss: 0.4694 - val_accuracy: 0.8241\n",
            "Epoch 304/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3132 - accuracy: 0.8886 - val_loss: 0.4737 - val_accuracy: 0.8362\n",
            "Epoch 305/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3032 - accuracy: 0.8937 - val_loss: 0.4965 - val_accuracy: 0.8281\n",
            "Epoch 306/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3183 - accuracy: 0.8866 - val_loss: 0.4915 - val_accuracy: 0.8212\n",
            "Epoch 307/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3108 - accuracy: 0.8928 - val_loss: 0.4654 - val_accuracy: 0.8403\n",
            "Epoch 308/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3151 - accuracy: 0.8877 - val_loss: 0.4823 - val_accuracy: 0.8287\n",
            "Epoch 309/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3049 - accuracy: 0.8931 - val_loss: 0.4796 - val_accuracy: 0.8299\n",
            "Epoch 310/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3051 - accuracy: 0.8960 - val_loss: 0.4800 - val_accuracy: 0.8230\n",
            "Epoch 311/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3053 - accuracy: 0.8943 - val_loss: 0.4661 - val_accuracy: 0.8345\n",
            "Epoch 312/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3145 - accuracy: 0.8854 - val_loss: 0.4821 - val_accuracy: 0.8345\n",
            "Epoch 313/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3046 - accuracy: 0.8860 - val_loss: 0.5005 - val_accuracy: 0.8253\n",
            "Epoch 314/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2998 - accuracy: 0.8991 - val_loss: 0.4575 - val_accuracy: 0.8368\n",
            "Epoch 315/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3033 - accuracy: 0.8889 - val_loss: 0.4678 - val_accuracy: 0.8304\n",
            "Epoch 316/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2975 - accuracy: 0.8980 - val_loss: 0.4577 - val_accuracy: 0.8368\n",
            "Epoch 317/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2949 - accuracy: 0.8891 - val_loss: 0.4770 - val_accuracy: 0.8385\n",
            "Epoch 318/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2912 - accuracy: 0.8954 - val_loss: 0.4879 - val_accuracy: 0.8299\n",
            "Epoch 319/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2986 - accuracy: 0.8971 - val_loss: 0.4773 - val_accuracy: 0.8310\n",
            "Epoch 320/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3016 - accuracy: 0.8928 - val_loss: 0.4811 - val_accuracy: 0.8379\n",
            "Epoch 321/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3051 - accuracy: 0.8835 - val_loss: 0.4689 - val_accuracy: 0.8339\n",
            "Epoch 322/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3003 - accuracy: 0.8894 - val_loss: 0.4787 - val_accuracy: 0.8241\n",
            "Epoch 323/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2967 - accuracy: 0.8872 - val_loss: 0.4805 - val_accuracy: 0.8287\n",
            "Epoch 324/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3019 - accuracy: 0.8937 - val_loss: 0.4953 - val_accuracy: 0.8189\n",
            "Epoch 325/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2905 - accuracy: 0.8994 - val_loss: 0.4591 - val_accuracy: 0.8333\n",
            "Epoch 326/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2930 - accuracy: 0.9051 - val_loss: 0.4734 - val_accuracy: 0.8328\n",
            "Epoch 327/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2903 - accuracy: 0.8985 - val_loss: 0.4598 - val_accuracy: 0.8345\n",
            "Epoch 328/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2990 - accuracy: 0.8948 - val_loss: 0.4566 - val_accuracy: 0.8345\n",
            "Epoch 329/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2963 - accuracy: 0.8948 - val_loss: 0.4625 - val_accuracy: 0.8379\n",
            "Epoch 330/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2909 - accuracy: 0.8965 - val_loss: 0.4652 - val_accuracy: 0.8328\n",
            "Epoch 331/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2912 - accuracy: 0.8982 - val_loss: 0.4716 - val_accuracy: 0.8293\n",
            "Epoch 332/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2971 - accuracy: 0.8926 - val_loss: 0.4599 - val_accuracy: 0.8385\n",
            "Epoch 333/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.3063 - accuracy: 0.8914 - val_loss: 0.4734 - val_accuracy: 0.8316\n",
            "Epoch 334/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2857 - accuracy: 0.8957 - val_loss: 0.4891 - val_accuracy: 0.8281\n",
            "Epoch 335/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2965 - accuracy: 0.8943 - val_loss: 0.4664 - val_accuracy: 0.8333\n",
            "Epoch 336/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2991 - accuracy: 0.8971 - val_loss: 0.4816 - val_accuracy: 0.8328\n",
            "Epoch 337/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2807 - accuracy: 0.8988 - val_loss: 0.4865 - val_accuracy: 0.8264\n",
            "Epoch 338/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2827 - accuracy: 0.8999 - val_loss: 0.4714 - val_accuracy: 0.8379\n",
            "Epoch 339/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.2839 - accuracy: 0.8937 - val_loss: 0.4587 - val_accuracy: 0.8385\n",
            "Epoch 340/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2873 - accuracy: 0.9031 - val_loss: 0.4731 - val_accuracy: 0.8403\n",
            "Epoch 341/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2942 - accuracy: 0.8948 - val_loss: 0.4745 - val_accuracy: 0.8328\n",
            "Epoch 342/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2928 - accuracy: 0.8994 - val_loss: 0.4594 - val_accuracy: 0.8391\n",
            "Epoch 343/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2748 - accuracy: 0.9019 - val_loss: 0.4770 - val_accuracy: 0.8333\n",
            "Epoch 344/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2863 - accuracy: 0.8960 - val_loss: 0.4826 - val_accuracy: 0.8224\n",
            "Epoch 345/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2851 - accuracy: 0.8982 - val_loss: 0.4666 - val_accuracy: 0.8333\n",
            "Epoch 346/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2855 - accuracy: 0.9002 - val_loss: 0.4787 - val_accuracy: 0.8362\n",
            "Epoch 347/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2754 - accuracy: 0.9031 - val_loss: 0.4901 - val_accuracy: 0.8253\n",
            "Epoch 348/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2868 - accuracy: 0.9028 - val_loss: 0.4636 - val_accuracy: 0.8356\n",
            "Epoch 349/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2839 - accuracy: 0.8960 - val_loss: 0.4654 - val_accuracy: 0.8362\n",
            "Epoch 350/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2719 - accuracy: 0.9059 - val_loss: 0.4743 - val_accuracy: 0.8345\n",
            "Epoch 351/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2771 - accuracy: 0.9039 - val_loss: 0.4697 - val_accuracy: 0.8351\n",
            "Epoch 352/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2793 - accuracy: 0.8985 - val_loss: 0.4703 - val_accuracy: 0.8276\n",
            "Epoch 353/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.2837 - accuracy: 0.9048 - val_loss: 0.4585 - val_accuracy: 0.8483\n",
            "Epoch 354/1000\n",
            "220/220 [==============================] - 2s 9ms/step - loss: 0.2723 - accuracy: 0.9014 - val_loss: 0.4688 - val_accuracy: 0.8391\n",
            "Epoch 355/1000\n",
            "220/220 [==============================] - 1s 7ms/step - loss: 0.2740 - accuracy: 0.9022 - val_loss: 0.4810 - val_accuracy: 0.8293\n",
            "Epoch 356/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2812 - accuracy: 0.8991 - val_loss: 0.4756 - val_accuracy: 0.8316\n",
            "Epoch 357/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2632 - accuracy: 0.9048 - val_loss: 0.4755 - val_accuracy: 0.8374\n",
            "Epoch 358/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2784 - accuracy: 0.9022 - val_loss: 0.4769 - val_accuracy: 0.8322\n",
            "Epoch 359/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2795 - accuracy: 0.8999 - val_loss: 0.4766 - val_accuracy: 0.8281\n",
            "Epoch 360/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2713 - accuracy: 0.9031 - val_loss: 0.4761 - val_accuracy: 0.8328\n",
            "Epoch 361/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2690 - accuracy: 0.9068 - val_loss: 0.4596 - val_accuracy: 0.8374\n",
            "Epoch 362/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2776 - accuracy: 0.8985 - val_loss: 0.4707 - val_accuracy: 0.8322\n",
            "Epoch 363/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2688 - accuracy: 0.9059 - val_loss: 0.4677 - val_accuracy: 0.8385\n",
            "Epoch 364/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.2870 - accuracy: 0.8988 - val_loss: 0.4675 - val_accuracy: 0.8397\n",
            "Epoch 365/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2671 - accuracy: 0.9022 - val_loss: 0.4723 - val_accuracy: 0.8403\n",
            "Epoch 366/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2704 - accuracy: 0.9056 - val_loss: 0.4627 - val_accuracy: 0.8403\n",
            "Epoch 367/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2768 - accuracy: 0.9005 - val_loss: 0.4638 - val_accuracy: 0.8356\n",
            "Epoch 368/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2694 - accuracy: 0.9045 - val_loss: 0.4678 - val_accuracy: 0.8403\n",
            "Epoch 369/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2635 - accuracy: 0.9119 - val_loss: 0.4669 - val_accuracy: 0.8379\n",
            "Epoch 370/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2695 - accuracy: 0.9045 - val_loss: 0.4681 - val_accuracy: 0.8466\n",
            "Epoch 371/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2637 - accuracy: 0.9105 - val_loss: 0.4665 - val_accuracy: 0.8391\n",
            "Epoch 372/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2694 - accuracy: 0.9093 - val_loss: 0.4582 - val_accuracy: 0.8420\n",
            "Epoch 373/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2623 - accuracy: 0.9102 - val_loss: 0.4961 - val_accuracy: 0.8247\n",
            "Epoch 374/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2710 - accuracy: 0.9031 - val_loss: 0.4591 - val_accuracy: 0.8420\n",
            "Epoch 375/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2577 - accuracy: 0.9093 - val_loss: 0.4649 - val_accuracy: 0.8414\n",
            "Epoch 376/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2535 - accuracy: 0.9088 - val_loss: 0.4658 - val_accuracy: 0.8449\n",
            "Epoch 377/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2624 - accuracy: 0.9065 - val_loss: 0.4829 - val_accuracy: 0.8333\n",
            "Epoch 378/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2732 - accuracy: 0.9048 - val_loss: 0.4663 - val_accuracy: 0.8368\n",
            "Epoch 379/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2581 - accuracy: 0.9051 - val_loss: 0.4771 - val_accuracy: 0.8218\n",
            "Epoch 380/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2602 - accuracy: 0.9088 - val_loss: 0.4736 - val_accuracy: 0.8356\n",
            "Epoch 381/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2535 - accuracy: 0.9167 - val_loss: 0.4636 - val_accuracy: 0.8299\n",
            "Epoch 382/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2633 - accuracy: 0.9090 - val_loss: 0.4788 - val_accuracy: 0.8333\n",
            "Epoch 383/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2680 - accuracy: 0.9073 - val_loss: 0.4650 - val_accuracy: 0.8328\n",
            "Epoch 384/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2659 - accuracy: 0.9025 - val_loss: 0.4954 - val_accuracy: 0.8339\n",
            "Epoch 385/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2644 - accuracy: 0.9031 - val_loss: 0.4766 - val_accuracy: 0.8397\n",
            "Epoch 386/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2598 - accuracy: 0.9110 - val_loss: 0.4649 - val_accuracy: 0.8408\n",
            "Epoch 387/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2526 - accuracy: 0.9110 - val_loss: 0.4675 - val_accuracy: 0.8391\n",
            "Epoch 388/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2627 - accuracy: 0.9065 - val_loss: 0.4631 - val_accuracy: 0.8420\n",
            "Epoch 389/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2495 - accuracy: 0.9215 - val_loss: 0.4605 - val_accuracy: 0.8414\n",
            "Epoch 390/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2653 - accuracy: 0.9039 - val_loss: 0.4793 - val_accuracy: 0.8287\n",
            "Epoch 391/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2536 - accuracy: 0.9082 - val_loss: 0.4635 - val_accuracy: 0.8356\n",
            "Epoch 392/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2591 - accuracy: 0.9099 - val_loss: 0.4645 - val_accuracy: 0.8304\n",
            "Epoch 393/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2503 - accuracy: 0.9093 - val_loss: 0.4620 - val_accuracy: 0.8310\n",
            "Epoch 394/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2559 - accuracy: 0.9153 - val_loss: 0.4640 - val_accuracy: 0.8356\n",
            "Epoch 395/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2485 - accuracy: 0.9110 - val_loss: 0.4634 - val_accuracy: 0.8431\n",
            "Epoch 396/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2565 - accuracy: 0.9130 - val_loss: 0.4607 - val_accuracy: 0.8374\n",
            "Epoch 397/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2509 - accuracy: 0.9082 - val_loss: 0.4964 - val_accuracy: 0.8224\n",
            "Epoch 398/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2628 - accuracy: 0.9042 - val_loss: 0.4638 - val_accuracy: 0.8385\n",
            "Epoch 399/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2471 - accuracy: 0.9156 - val_loss: 0.4521 - val_accuracy: 0.8426\n",
            "Epoch 400/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2477 - accuracy: 0.9096 - val_loss: 0.4778 - val_accuracy: 0.8426\n",
            "Epoch 401/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2338 - accuracy: 0.9119 - val_loss: 0.4747 - val_accuracy: 0.8293\n",
            "Epoch 402/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2500 - accuracy: 0.9116 - val_loss: 0.4774 - val_accuracy: 0.8368\n",
            "Epoch 403/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2578 - accuracy: 0.9113 - val_loss: 0.4899 - val_accuracy: 0.8328\n",
            "Epoch 404/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2394 - accuracy: 0.9133 - val_loss: 0.4628 - val_accuracy: 0.8472\n",
            "Epoch 405/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2404 - accuracy: 0.9127 - val_loss: 0.4589 - val_accuracy: 0.8437\n",
            "Epoch 406/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2470 - accuracy: 0.9127 - val_loss: 0.4889 - val_accuracy: 0.8293\n",
            "Epoch 407/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2589 - accuracy: 0.9105 - val_loss: 0.4641 - val_accuracy: 0.8379\n",
            "Epoch 408/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2326 - accuracy: 0.9170 - val_loss: 0.4695 - val_accuracy: 0.8426\n",
            "Epoch 409/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2461 - accuracy: 0.9119 - val_loss: 0.4807 - val_accuracy: 0.8328\n",
            "Epoch 410/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2367 - accuracy: 0.9201 - val_loss: 0.4705 - val_accuracy: 0.8368\n",
            "Epoch 411/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2485 - accuracy: 0.9125 - val_loss: 0.4637 - val_accuracy: 0.8368\n",
            "Epoch 412/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2468 - accuracy: 0.9090 - val_loss: 0.4640 - val_accuracy: 0.8351\n",
            "Epoch 413/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2497 - accuracy: 0.9147 - val_loss: 0.4569 - val_accuracy: 0.8437\n",
            "Epoch 414/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2363 - accuracy: 0.9187 - val_loss: 0.4922 - val_accuracy: 0.8247\n",
            "Epoch 415/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2364 - accuracy: 0.9136 - val_loss: 0.4692 - val_accuracy: 0.8322\n",
            "Epoch 416/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2428 - accuracy: 0.9159 - val_loss: 0.4676 - val_accuracy: 0.8339\n",
            "Epoch 417/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2415 - accuracy: 0.9147 - val_loss: 0.4687 - val_accuracy: 0.8385\n",
            "Epoch 418/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2420 - accuracy: 0.9150 - val_loss: 0.4914 - val_accuracy: 0.8391\n",
            "Epoch 419/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2390 - accuracy: 0.9136 - val_loss: 0.4467 - val_accuracy: 0.8379\n",
            "Epoch 420/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2305 - accuracy: 0.9176 - val_loss: 0.4736 - val_accuracy: 0.8414\n",
            "Epoch 421/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2356 - accuracy: 0.9181 - val_loss: 0.4578 - val_accuracy: 0.8431\n",
            "Epoch 422/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2425 - accuracy: 0.9164 - val_loss: 0.4762 - val_accuracy: 0.8368\n",
            "Epoch 423/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2255 - accuracy: 0.9221 - val_loss: 0.4710 - val_accuracy: 0.8339\n",
            "Epoch 424/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2285 - accuracy: 0.9119 - val_loss: 0.4764 - val_accuracy: 0.8264\n",
            "Epoch 425/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2442 - accuracy: 0.9116 - val_loss: 0.4753 - val_accuracy: 0.8281\n",
            "Epoch 426/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2433 - accuracy: 0.9099 - val_loss: 0.4655 - val_accuracy: 0.8403\n",
            "Epoch 427/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2306 - accuracy: 0.9190 - val_loss: 0.4765 - val_accuracy: 0.8449\n",
            "Epoch 428/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2421 - accuracy: 0.9125 - val_loss: 0.4888 - val_accuracy: 0.8287\n",
            "Epoch 429/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2344 - accuracy: 0.9176 - val_loss: 0.4791 - val_accuracy: 0.8362\n",
            "Epoch 430/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2383 - accuracy: 0.9164 - val_loss: 0.4671 - val_accuracy: 0.8426\n",
            "Epoch 431/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2289 - accuracy: 0.9230 - val_loss: 0.4881 - val_accuracy: 0.8379\n",
            "Epoch 432/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2337 - accuracy: 0.9201 - val_loss: 0.4593 - val_accuracy: 0.8443\n",
            "Epoch 433/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2293 - accuracy: 0.9193 - val_loss: 0.4744 - val_accuracy: 0.8397\n",
            "Epoch 434/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2282 - accuracy: 0.9193 - val_loss: 0.4756 - val_accuracy: 0.8356\n",
            "Epoch 435/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2351 - accuracy: 0.9204 - val_loss: 0.4762 - val_accuracy: 0.8368\n",
            "Epoch 436/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2232 - accuracy: 0.9233 - val_loss: 0.4828 - val_accuracy: 0.8466\n",
            "Epoch 437/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2373 - accuracy: 0.9167 - val_loss: 0.4765 - val_accuracy: 0.8385\n",
            "Epoch 438/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2300 - accuracy: 0.9184 - val_loss: 0.4640 - val_accuracy: 0.8443\n",
            "Epoch 439/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2402 - accuracy: 0.9193 - val_loss: 0.4535 - val_accuracy: 0.8403\n",
            "Epoch 440/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2184 - accuracy: 0.9255 - val_loss: 0.4803 - val_accuracy: 0.8322\n",
            "Epoch 441/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2366 - accuracy: 0.9153 - val_loss: 0.4704 - val_accuracy: 0.8345\n",
            "Epoch 442/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2276 - accuracy: 0.9227 - val_loss: 0.4856 - val_accuracy: 0.8339\n",
            "Epoch 443/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2261 - accuracy: 0.9184 - val_loss: 0.4660 - val_accuracy: 0.8379\n",
            "Epoch 444/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2273 - accuracy: 0.9196 - val_loss: 0.4597 - val_accuracy: 0.8304\n",
            "Epoch 445/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2246 - accuracy: 0.9244 - val_loss: 0.4692 - val_accuracy: 0.8408\n",
            "Epoch 446/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2223 - accuracy: 0.9176 - val_loss: 0.4696 - val_accuracy: 0.8328\n",
            "Epoch 447/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2324 - accuracy: 0.9201 - val_loss: 0.4660 - val_accuracy: 0.8460\n",
            "Epoch 448/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2278 - accuracy: 0.9170 - val_loss: 0.4674 - val_accuracy: 0.8478\n",
            "Epoch 449/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2126 - accuracy: 0.9235 - val_loss: 0.4758 - val_accuracy: 0.8408\n",
            "Epoch 450/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2246 - accuracy: 0.9230 - val_loss: 0.4709 - val_accuracy: 0.8478\n",
            "Epoch 451/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2255 - accuracy: 0.9204 - val_loss: 0.4610 - val_accuracy: 0.8466\n",
            "Epoch 452/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2313 - accuracy: 0.9176 - val_loss: 0.4649 - val_accuracy: 0.8426\n",
            "Epoch 453/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2256 - accuracy: 0.9224 - val_loss: 0.4665 - val_accuracy: 0.8466\n",
            "Epoch 454/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2197 - accuracy: 0.9201 - val_loss: 0.4807 - val_accuracy: 0.8414\n",
            "Epoch 455/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2295 - accuracy: 0.9204 - val_loss: 0.4778 - val_accuracy: 0.8316\n",
            "Epoch 456/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2266 - accuracy: 0.9196 - val_loss: 0.4789 - val_accuracy: 0.8316\n",
            "Epoch 457/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2186 - accuracy: 0.9198 - val_loss: 0.4819 - val_accuracy: 0.8362\n",
            "Epoch 458/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2227 - accuracy: 0.9207 - val_loss: 0.4705 - val_accuracy: 0.8403\n",
            "Epoch 459/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2091 - accuracy: 0.9278 - val_loss: 0.4702 - val_accuracy: 0.8431\n",
            "Epoch 460/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2297 - accuracy: 0.9161 - val_loss: 0.4595 - val_accuracy: 0.8524\n",
            "Epoch 461/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2258 - accuracy: 0.9227 - val_loss: 0.4712 - val_accuracy: 0.8437\n",
            "Epoch 462/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2148 - accuracy: 0.9210 - val_loss: 0.4788 - val_accuracy: 0.8506\n",
            "Epoch 463/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2251 - accuracy: 0.9218 - val_loss: 0.4622 - val_accuracy: 0.8397\n",
            "Epoch 464/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2252 - accuracy: 0.9215 - val_loss: 0.4656 - val_accuracy: 0.8391\n",
            "Epoch 465/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2041 - accuracy: 0.9275 - val_loss: 0.4587 - val_accuracy: 0.8356\n",
            "Epoch 466/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2062 - accuracy: 0.9318 - val_loss: 0.4715 - val_accuracy: 0.8368\n",
            "Epoch 467/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2224 - accuracy: 0.9255 - val_loss: 0.4627 - val_accuracy: 0.8403\n",
            "Epoch 468/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2272 - accuracy: 0.9196 - val_loss: 0.4806 - val_accuracy: 0.8403\n",
            "Epoch 469/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2206 - accuracy: 0.9204 - val_loss: 0.4665 - val_accuracy: 0.8472\n",
            "Epoch 470/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2162 - accuracy: 0.9210 - val_loss: 0.4723 - val_accuracy: 0.8466\n",
            "Epoch 471/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2159 - accuracy: 0.9255 - val_loss: 0.4671 - val_accuracy: 0.8414\n",
            "Epoch 472/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2181 - accuracy: 0.9235 - val_loss: 0.4697 - val_accuracy: 0.8431\n",
            "Epoch 473/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2132 - accuracy: 0.9278 - val_loss: 0.4746 - val_accuracy: 0.8351\n",
            "Epoch 474/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2057 - accuracy: 0.9295 - val_loss: 0.4707 - val_accuracy: 0.8356\n",
            "Epoch 475/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2048 - accuracy: 0.9258 - val_loss: 0.4823 - val_accuracy: 0.8403\n",
            "Epoch 476/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2218 - accuracy: 0.9201 - val_loss: 0.4797 - val_accuracy: 0.8397\n",
            "Epoch 477/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2133 - accuracy: 0.9261 - val_loss: 0.4823 - val_accuracy: 0.8258\n",
            "Epoch 478/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2247 - accuracy: 0.9227 - val_loss: 0.5030 - val_accuracy: 0.8264\n",
            "Epoch 479/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2193 - accuracy: 0.9281 - val_loss: 0.4478 - val_accuracy: 0.8529\n",
            "Epoch 480/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2086 - accuracy: 0.9224 - val_loss: 0.4970 - val_accuracy: 0.8397\n",
            "Epoch 481/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2134 - accuracy: 0.9215 - val_loss: 0.4656 - val_accuracy: 0.8420\n",
            "Epoch 482/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2083 - accuracy: 0.9261 - val_loss: 0.4763 - val_accuracy: 0.8374\n",
            "Epoch 483/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2057 - accuracy: 0.9269 - val_loss: 0.4605 - val_accuracy: 0.8431\n",
            "Epoch 484/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2046 - accuracy: 0.9255 - val_loss: 0.4774 - val_accuracy: 0.8408\n",
            "Epoch 485/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2082 - accuracy: 0.9241 - val_loss: 0.4657 - val_accuracy: 0.8385\n",
            "Epoch 486/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2034 - accuracy: 0.9312 - val_loss: 0.4910 - val_accuracy: 0.8368\n",
            "Epoch 487/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2031 - accuracy: 0.9301 - val_loss: 0.4674 - val_accuracy: 0.8460\n",
            "Epoch 488/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2124 - accuracy: 0.9284 - val_loss: 0.4837 - val_accuracy: 0.8379\n",
            "Epoch 489/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1972 - accuracy: 0.9289 - val_loss: 0.4724 - val_accuracy: 0.8362\n",
            "Epoch 490/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2104 - accuracy: 0.9264 - val_loss: 0.4793 - val_accuracy: 0.8391\n",
            "Epoch 491/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2025 - accuracy: 0.9335 - val_loss: 0.4660 - val_accuracy: 0.8420\n",
            "Epoch 492/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2123 - accuracy: 0.9295 - val_loss: 0.4699 - val_accuracy: 0.8454\n",
            "Epoch 493/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1965 - accuracy: 0.9287 - val_loss: 0.4593 - val_accuracy: 0.8408\n",
            "Epoch 494/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2090 - accuracy: 0.9247 - val_loss: 0.4675 - val_accuracy: 0.8437\n",
            "Epoch 495/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1981 - accuracy: 0.9338 - val_loss: 0.4815 - val_accuracy: 0.8408\n",
            "Epoch 496/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2110 - accuracy: 0.9269 - val_loss: 0.4764 - val_accuracy: 0.8466\n",
            "Epoch 497/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2094 - accuracy: 0.9284 - val_loss: 0.4843 - val_accuracy: 0.8356\n",
            "Epoch 498/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2062 - accuracy: 0.9261 - val_loss: 0.4771 - val_accuracy: 0.8437\n",
            "Epoch 499/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2046 - accuracy: 0.9295 - val_loss: 0.4680 - val_accuracy: 0.8379\n",
            "Epoch 500/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1974 - accuracy: 0.9343 - val_loss: 0.4672 - val_accuracy: 0.8426\n",
            "Epoch 501/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1985 - accuracy: 0.9269 - val_loss: 0.4738 - val_accuracy: 0.8454\n",
            "Epoch 502/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1816 - accuracy: 0.9332 - val_loss: 0.4915 - val_accuracy: 0.8356\n",
            "Epoch 503/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1982 - accuracy: 0.9292 - val_loss: 0.4717 - val_accuracy: 0.8431\n",
            "Epoch 504/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1953 - accuracy: 0.9287 - val_loss: 0.4758 - val_accuracy: 0.8379\n",
            "Epoch 505/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2058 - accuracy: 0.9275 - val_loss: 0.4830 - val_accuracy: 0.8420\n",
            "Epoch 506/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1923 - accuracy: 0.9323 - val_loss: 0.4794 - val_accuracy: 0.8506\n",
            "Epoch 507/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1874 - accuracy: 0.9377 - val_loss: 0.4987 - val_accuracy: 0.8437\n",
            "Epoch 508/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.2011 - accuracy: 0.9295 - val_loss: 0.4756 - val_accuracy: 0.8408\n",
            "Epoch 509/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1993 - accuracy: 0.9281 - val_loss: 0.4942 - val_accuracy: 0.8356\n",
            "Epoch 510/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1964 - accuracy: 0.9343 - val_loss: 0.4817 - val_accuracy: 0.8362\n",
            "Epoch 511/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1999 - accuracy: 0.9278 - val_loss: 0.4793 - val_accuracy: 0.8345\n",
            "Epoch 512/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2022 - accuracy: 0.9309 - val_loss: 0.4599 - val_accuracy: 0.8449\n",
            "Epoch 513/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1878 - accuracy: 0.9346 - val_loss: 0.4649 - val_accuracy: 0.8524\n",
            "Epoch 514/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1957 - accuracy: 0.9323 - val_loss: 0.4761 - val_accuracy: 0.8368\n",
            "Epoch 515/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2009 - accuracy: 0.9298 - val_loss: 0.4783 - val_accuracy: 0.8414\n",
            "Epoch 516/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1959 - accuracy: 0.9306 - val_loss: 0.4811 - val_accuracy: 0.8374\n",
            "Epoch 517/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2030 - accuracy: 0.9309 - val_loss: 0.4625 - val_accuracy: 0.8443\n",
            "Epoch 518/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.2013 - accuracy: 0.9312 - val_loss: 0.4722 - val_accuracy: 0.8397\n",
            "Epoch 519/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.2001 - accuracy: 0.9301 - val_loss: 0.4898 - val_accuracy: 0.8449\n",
            "Epoch 520/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1875 - accuracy: 0.9360 - val_loss: 0.4887 - val_accuracy: 0.8333\n",
            "Epoch 521/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1977 - accuracy: 0.9309 - val_loss: 0.4743 - val_accuracy: 0.8431\n",
            "Epoch 522/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1935 - accuracy: 0.9306 - val_loss: 0.4741 - val_accuracy: 0.8408\n",
            "Epoch 523/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1894 - accuracy: 0.9267 - val_loss: 0.4763 - val_accuracy: 0.8437\n",
            "Epoch 524/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1892 - accuracy: 0.9309 - val_loss: 0.4810 - val_accuracy: 0.8420\n",
            "Epoch 525/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1948 - accuracy: 0.9346 - val_loss: 0.4845 - val_accuracy: 0.8270\n",
            "Epoch 526/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1873 - accuracy: 0.9326 - val_loss: 0.4686 - val_accuracy: 0.8437\n",
            "Epoch 527/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.2025 - accuracy: 0.9275 - val_loss: 0.4833 - val_accuracy: 0.8379\n",
            "Epoch 528/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1934 - accuracy: 0.9309 - val_loss: 0.4724 - val_accuracy: 0.8466\n",
            "Epoch 529/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1911 - accuracy: 0.9315 - val_loss: 0.4743 - val_accuracy: 0.8420\n",
            "Epoch 530/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1890 - accuracy: 0.9346 - val_loss: 0.4890 - val_accuracy: 0.8454\n",
            "Epoch 531/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1873 - accuracy: 0.9329 - val_loss: 0.4977 - val_accuracy: 0.8408\n",
            "Epoch 532/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1738 - accuracy: 0.9395 - val_loss: 0.4840 - val_accuracy: 0.8443\n",
            "Epoch 533/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1894 - accuracy: 0.9346 - val_loss: 0.4914 - val_accuracy: 0.8426\n",
            "Epoch 534/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1823 - accuracy: 0.9352 - val_loss: 0.4852 - val_accuracy: 0.8420\n",
            "Epoch 535/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1908 - accuracy: 0.9332 - val_loss: 0.4858 - val_accuracy: 0.8362\n",
            "Epoch 536/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1984 - accuracy: 0.9267 - val_loss: 0.4652 - val_accuracy: 0.8437\n",
            "Epoch 537/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1873 - accuracy: 0.9323 - val_loss: 0.4737 - val_accuracy: 0.8478\n",
            "Epoch 538/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1833 - accuracy: 0.9323 - val_loss: 0.4960 - val_accuracy: 0.8293\n",
            "Epoch 539/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1896 - accuracy: 0.9343 - val_loss: 0.4632 - val_accuracy: 0.8414\n",
            "Epoch 540/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1800 - accuracy: 0.9406 - val_loss: 0.4703 - val_accuracy: 0.8437\n",
            "Epoch 541/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1908 - accuracy: 0.9383 - val_loss: 0.4713 - val_accuracy: 0.8466\n",
            "Epoch 542/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1823 - accuracy: 0.9375 - val_loss: 0.4861 - val_accuracy: 0.8333\n",
            "Epoch 543/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1758 - accuracy: 0.9389 - val_loss: 0.4732 - val_accuracy: 0.8426\n",
            "Epoch 544/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1913 - accuracy: 0.9338 - val_loss: 0.4819 - val_accuracy: 0.8385\n",
            "Epoch 545/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1802 - accuracy: 0.9389 - val_loss: 0.4714 - val_accuracy: 0.8391\n",
            "Epoch 546/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1851 - accuracy: 0.9369 - val_loss: 0.4785 - val_accuracy: 0.8454\n",
            "Epoch 547/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1831 - accuracy: 0.9355 - val_loss: 0.4830 - val_accuracy: 0.8449\n",
            "Epoch 548/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1877 - accuracy: 0.9346 - val_loss: 0.4709 - val_accuracy: 0.8518\n",
            "Epoch 549/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1883 - accuracy: 0.9338 - val_loss: 0.5162 - val_accuracy: 0.8224\n",
            "Epoch 550/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1805 - accuracy: 0.9375 - val_loss: 0.4792 - val_accuracy: 0.8408\n",
            "Epoch 551/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1735 - accuracy: 0.9358 - val_loss: 0.4972 - val_accuracy: 0.8351\n",
            "Epoch 552/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1825 - accuracy: 0.9400 - val_loss: 0.4764 - val_accuracy: 0.8437\n",
            "Epoch 553/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1916 - accuracy: 0.9321 - val_loss: 0.4661 - val_accuracy: 0.8437\n",
            "Epoch 554/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1843 - accuracy: 0.9346 - val_loss: 0.5058 - val_accuracy: 0.8316\n",
            "Epoch 555/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1906 - accuracy: 0.9304 - val_loss: 0.4867 - val_accuracy: 0.8351\n",
            "Epoch 556/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1920 - accuracy: 0.9295 - val_loss: 0.4822 - val_accuracy: 0.8397\n",
            "Epoch 557/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1766 - accuracy: 0.9372 - val_loss: 0.4836 - val_accuracy: 0.8420\n",
            "Epoch 558/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1930 - accuracy: 0.9315 - val_loss: 0.4874 - val_accuracy: 0.8345\n",
            "Epoch 559/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1796 - accuracy: 0.9375 - val_loss: 0.5031 - val_accuracy: 0.8351\n",
            "Epoch 560/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1762 - accuracy: 0.9417 - val_loss: 0.4926 - val_accuracy: 0.8310\n",
            "Epoch 561/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1747 - accuracy: 0.9369 - val_loss: 0.4678 - val_accuracy: 0.8501\n",
            "Epoch 562/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1750 - accuracy: 0.9389 - val_loss: 0.4836 - val_accuracy: 0.8489\n",
            "Epoch 563/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1850 - accuracy: 0.9358 - val_loss: 0.4689 - val_accuracy: 0.8397\n",
            "Epoch 564/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1757 - accuracy: 0.9380 - val_loss: 0.4633 - val_accuracy: 0.8431\n",
            "Epoch 565/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1716 - accuracy: 0.9383 - val_loss: 0.4925 - val_accuracy: 0.8431\n",
            "Epoch 566/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1660 - accuracy: 0.9449 - val_loss: 0.4893 - val_accuracy: 0.8443\n",
            "Epoch 567/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1677 - accuracy: 0.9420 - val_loss: 0.4754 - val_accuracy: 0.8460\n",
            "Epoch 568/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1721 - accuracy: 0.9392 - val_loss: 0.4655 - val_accuracy: 0.8379\n",
            "Epoch 569/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1758 - accuracy: 0.9369 - val_loss: 0.4848 - val_accuracy: 0.8466\n",
            "Epoch 570/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1740 - accuracy: 0.9400 - val_loss: 0.4815 - val_accuracy: 0.8449\n",
            "Epoch 571/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1849 - accuracy: 0.9349 - val_loss: 0.4698 - val_accuracy: 0.8362\n",
            "Epoch 572/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1779 - accuracy: 0.9363 - val_loss: 0.4693 - val_accuracy: 0.8552\n",
            "Epoch 573/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1788 - accuracy: 0.9366 - val_loss: 0.4794 - val_accuracy: 0.8385\n",
            "Epoch 574/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1767 - accuracy: 0.9412 - val_loss: 0.4673 - val_accuracy: 0.8449\n",
            "Epoch 575/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1716 - accuracy: 0.9414 - val_loss: 0.4882 - val_accuracy: 0.8478\n",
            "Epoch 576/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1722 - accuracy: 0.9369 - val_loss: 0.4855 - val_accuracy: 0.8431\n",
            "Epoch 577/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1638 - accuracy: 0.9434 - val_loss: 0.4835 - val_accuracy: 0.8356\n",
            "Epoch 578/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1678 - accuracy: 0.9406 - val_loss: 0.4883 - val_accuracy: 0.8437\n",
            "Epoch 579/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1781 - accuracy: 0.9386 - val_loss: 0.4730 - val_accuracy: 0.8472\n",
            "Epoch 580/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1628 - accuracy: 0.9463 - val_loss: 0.5227 - val_accuracy: 0.8322\n",
            "Epoch 581/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1727 - accuracy: 0.9412 - val_loss: 0.4844 - val_accuracy: 0.8437\n",
            "Epoch 582/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1671 - accuracy: 0.9417 - val_loss: 0.5040 - val_accuracy: 0.8356\n",
            "Epoch 583/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1653 - accuracy: 0.9412 - val_loss: 0.4687 - val_accuracy: 0.8437\n",
            "Epoch 584/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1763 - accuracy: 0.9375 - val_loss: 0.4921 - val_accuracy: 0.8385\n",
            "Epoch 585/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1710 - accuracy: 0.9355 - val_loss: 0.4842 - val_accuracy: 0.8501\n",
            "Epoch 586/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1695 - accuracy: 0.9397 - val_loss: 0.4950 - val_accuracy: 0.8408\n",
            "Epoch 587/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1687 - accuracy: 0.9417 - val_loss: 0.4834 - val_accuracy: 0.8454\n",
            "Epoch 588/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1658 - accuracy: 0.9434 - val_loss: 0.4626 - val_accuracy: 0.8397\n",
            "Epoch 589/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1765 - accuracy: 0.9338 - val_loss: 0.4787 - val_accuracy: 0.8443\n",
            "Epoch 590/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1740 - accuracy: 0.9412 - val_loss: 0.4961 - val_accuracy: 0.8420\n",
            "Epoch 591/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1631 - accuracy: 0.9429 - val_loss: 0.4910 - val_accuracy: 0.8431\n",
            "Epoch 592/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1606 - accuracy: 0.9443 - val_loss: 0.5053 - val_accuracy: 0.8512\n",
            "Epoch 593/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1620 - accuracy: 0.9420 - val_loss: 0.4846 - val_accuracy: 0.8443\n",
            "Epoch 594/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1679 - accuracy: 0.9417 - val_loss: 0.4699 - val_accuracy: 0.8501\n",
            "Epoch 595/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1754 - accuracy: 0.9383 - val_loss: 0.4918 - val_accuracy: 0.8437\n",
            "Epoch 596/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1803 - accuracy: 0.9335 - val_loss: 0.4845 - val_accuracy: 0.8426\n",
            "Epoch 597/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1692 - accuracy: 0.9451 - val_loss: 0.4955 - val_accuracy: 0.8385\n",
            "Epoch 598/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1737 - accuracy: 0.9426 - val_loss: 0.4940 - val_accuracy: 0.8472\n",
            "Epoch 599/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1588 - accuracy: 0.9451 - val_loss: 0.4982 - val_accuracy: 0.8368\n",
            "Epoch 600/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1684 - accuracy: 0.9403 - val_loss: 0.4929 - val_accuracy: 0.8443\n",
            "Epoch 601/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1614 - accuracy: 0.9460 - val_loss: 0.4706 - val_accuracy: 0.8414\n",
            "Epoch 602/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1629 - accuracy: 0.9451 - val_loss: 0.4937 - val_accuracy: 0.8426\n",
            "Epoch 603/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1677 - accuracy: 0.9449 - val_loss: 0.4860 - val_accuracy: 0.8431\n",
            "Epoch 604/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1576 - accuracy: 0.9440 - val_loss: 0.4927 - val_accuracy: 0.8506\n",
            "Epoch 605/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1746 - accuracy: 0.9412 - val_loss: 0.4806 - val_accuracy: 0.8454\n",
            "Epoch 606/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1626 - accuracy: 0.9429 - val_loss: 0.4958 - val_accuracy: 0.8403\n",
            "Epoch 607/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1676 - accuracy: 0.9468 - val_loss: 0.5042 - val_accuracy: 0.8431\n",
            "Epoch 608/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1633 - accuracy: 0.9414 - val_loss: 0.4828 - val_accuracy: 0.8449\n",
            "Epoch 609/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1644 - accuracy: 0.9414 - val_loss: 0.4684 - val_accuracy: 0.8501\n",
            "Epoch 610/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1590 - accuracy: 0.9409 - val_loss: 0.4732 - val_accuracy: 0.8379\n",
            "Epoch 611/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1597 - accuracy: 0.9440 - val_loss: 0.4765 - val_accuracy: 0.8420\n",
            "Epoch 612/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1597 - accuracy: 0.9471 - val_loss: 0.5065 - val_accuracy: 0.8466\n",
            "Epoch 613/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1671 - accuracy: 0.9389 - val_loss: 0.4757 - val_accuracy: 0.8379\n",
            "Epoch 614/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1657 - accuracy: 0.9446 - val_loss: 0.4681 - val_accuracy: 0.8443\n",
            "Epoch 615/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1552 - accuracy: 0.9437 - val_loss: 0.4857 - val_accuracy: 0.8449\n",
            "Epoch 616/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1551 - accuracy: 0.9443 - val_loss: 0.4891 - val_accuracy: 0.8506\n",
            "Epoch 617/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1620 - accuracy: 0.9423 - val_loss: 0.4839 - val_accuracy: 0.8478\n",
            "Epoch 618/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1572 - accuracy: 0.9463 - val_loss: 0.4793 - val_accuracy: 0.8431\n",
            "Epoch 619/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1627 - accuracy: 0.9480 - val_loss: 0.4803 - val_accuracy: 0.8431\n",
            "Epoch 620/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1653 - accuracy: 0.9397 - val_loss: 0.4863 - val_accuracy: 0.8420\n",
            "Epoch 621/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1702 - accuracy: 0.9414 - val_loss: 0.4837 - val_accuracy: 0.8437\n",
            "Epoch 622/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1604 - accuracy: 0.9409 - val_loss: 0.5001 - val_accuracy: 0.8408\n",
            "Epoch 623/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1564 - accuracy: 0.9468 - val_loss: 0.4717 - val_accuracy: 0.8506\n",
            "Epoch 624/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1486 - accuracy: 0.9477 - val_loss: 0.4875 - val_accuracy: 0.8397\n",
            "Epoch 625/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1487 - accuracy: 0.9503 - val_loss: 0.5026 - val_accuracy: 0.8426\n",
            "Epoch 626/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1615 - accuracy: 0.9426 - val_loss: 0.4903 - val_accuracy: 0.8379\n",
            "Epoch 627/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1605 - accuracy: 0.9417 - val_loss: 0.5046 - val_accuracy: 0.8304\n",
            "Epoch 628/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1669 - accuracy: 0.9406 - val_loss: 0.5023 - val_accuracy: 0.8431\n",
            "Epoch 629/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1548 - accuracy: 0.9454 - val_loss: 0.5068 - val_accuracy: 0.8437\n",
            "Epoch 630/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1443 - accuracy: 0.9494 - val_loss: 0.5151 - val_accuracy: 0.8345\n",
            "Epoch 631/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1592 - accuracy: 0.9454 - val_loss: 0.4634 - val_accuracy: 0.8460\n",
            "Epoch 632/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1500 - accuracy: 0.9511 - val_loss: 0.4864 - val_accuracy: 0.8472\n",
            "Epoch 633/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1588 - accuracy: 0.9414 - val_loss: 0.4863 - val_accuracy: 0.8397\n",
            "Epoch 634/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1554 - accuracy: 0.9468 - val_loss: 0.4817 - val_accuracy: 0.8333\n",
            "Epoch 635/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1556 - accuracy: 0.9483 - val_loss: 0.5006 - val_accuracy: 0.8437\n",
            "Epoch 636/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1432 - accuracy: 0.9494 - val_loss: 0.4846 - val_accuracy: 0.8483\n",
            "Epoch 637/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1467 - accuracy: 0.9503 - val_loss: 0.4713 - val_accuracy: 0.8506\n",
            "Epoch 638/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1546 - accuracy: 0.9471 - val_loss: 0.4977 - val_accuracy: 0.8472\n",
            "Epoch 639/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1436 - accuracy: 0.9528 - val_loss: 0.4974 - val_accuracy: 0.8460\n",
            "Epoch 640/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1572 - accuracy: 0.9454 - val_loss: 0.4794 - val_accuracy: 0.8460\n",
            "Epoch 641/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1515 - accuracy: 0.9457 - val_loss: 0.4836 - val_accuracy: 0.8489\n",
            "Epoch 642/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1534 - accuracy: 0.9494 - val_loss: 0.4856 - val_accuracy: 0.8483\n",
            "Epoch 643/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1590 - accuracy: 0.9460 - val_loss: 0.4726 - val_accuracy: 0.8437\n",
            "Epoch 644/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1561 - accuracy: 0.9488 - val_loss: 0.4731 - val_accuracy: 0.8483\n",
            "Epoch 645/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1482 - accuracy: 0.9466 - val_loss: 0.4967 - val_accuracy: 0.8420\n",
            "Epoch 646/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1532 - accuracy: 0.9494 - val_loss: 0.4935 - val_accuracy: 0.8466\n",
            "Epoch 647/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1510 - accuracy: 0.9497 - val_loss: 0.5158 - val_accuracy: 0.8449\n",
            "Epoch 648/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1491 - accuracy: 0.9468 - val_loss: 0.5007 - val_accuracy: 0.8454\n",
            "Epoch 649/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1406 - accuracy: 0.9542 - val_loss: 0.5018 - val_accuracy: 0.8379\n",
            "Epoch 650/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1513 - accuracy: 0.9497 - val_loss: 0.5039 - val_accuracy: 0.8472\n",
            "Epoch 651/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1560 - accuracy: 0.9423 - val_loss: 0.5025 - val_accuracy: 0.8489\n",
            "Epoch 652/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1427 - accuracy: 0.9537 - val_loss: 0.5157 - val_accuracy: 0.8374\n",
            "Epoch 653/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1558 - accuracy: 0.9491 - val_loss: 0.4895 - val_accuracy: 0.8460\n",
            "Epoch 654/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1481 - accuracy: 0.9477 - val_loss: 0.4996 - val_accuracy: 0.8397\n",
            "Epoch 655/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1516 - accuracy: 0.9466 - val_loss: 0.5006 - val_accuracy: 0.8489\n",
            "Epoch 656/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1468 - accuracy: 0.9497 - val_loss: 0.5089 - val_accuracy: 0.8397\n",
            "Epoch 657/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1418 - accuracy: 0.9514 - val_loss: 0.4872 - val_accuracy: 0.8403\n",
            "Epoch 658/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1523 - accuracy: 0.9454 - val_loss: 0.4946 - val_accuracy: 0.8466\n",
            "Epoch 659/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1416 - accuracy: 0.9517 - val_loss: 0.5046 - val_accuracy: 0.8420\n",
            "Epoch 660/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1466 - accuracy: 0.9508 - val_loss: 0.5155 - val_accuracy: 0.8374\n",
            "Epoch 661/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1461 - accuracy: 0.9503 - val_loss: 0.5071 - val_accuracy: 0.8431\n",
            "Epoch 662/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1394 - accuracy: 0.9522 - val_loss: 0.4914 - val_accuracy: 0.8501\n",
            "Epoch 663/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1489 - accuracy: 0.9460 - val_loss: 0.5133 - val_accuracy: 0.8385\n",
            "Epoch 664/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1441 - accuracy: 0.9522 - val_loss: 0.4847 - val_accuracy: 0.8454\n",
            "Epoch 665/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1523 - accuracy: 0.9471 - val_loss: 0.4927 - val_accuracy: 0.8478\n",
            "Epoch 666/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1475 - accuracy: 0.9486 - val_loss: 0.4716 - val_accuracy: 0.8489\n",
            "Epoch 667/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1360 - accuracy: 0.9488 - val_loss: 0.5156 - val_accuracy: 0.8483\n",
            "Epoch 668/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1408 - accuracy: 0.9491 - val_loss: 0.4944 - val_accuracy: 0.8420\n",
            "Epoch 669/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1429 - accuracy: 0.9517 - val_loss: 0.4996 - val_accuracy: 0.8403\n",
            "Epoch 670/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1415 - accuracy: 0.9551 - val_loss: 0.5006 - val_accuracy: 0.8391\n",
            "Epoch 671/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1536 - accuracy: 0.9483 - val_loss: 0.4944 - val_accuracy: 0.8426\n",
            "Epoch 672/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1329 - accuracy: 0.9576 - val_loss: 0.4912 - val_accuracy: 0.8368\n",
            "Epoch 673/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1329 - accuracy: 0.9574 - val_loss: 0.4847 - val_accuracy: 0.8443\n",
            "Epoch 674/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1450 - accuracy: 0.9503 - val_loss: 0.4840 - val_accuracy: 0.8460\n",
            "Epoch 675/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1370 - accuracy: 0.9534 - val_loss: 0.4859 - val_accuracy: 0.8472\n",
            "Epoch 676/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1341 - accuracy: 0.9520 - val_loss: 0.4923 - val_accuracy: 0.8443\n",
            "Epoch 677/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1369 - accuracy: 0.9548 - val_loss: 0.4709 - val_accuracy: 0.8454\n",
            "Epoch 678/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1360 - accuracy: 0.9508 - val_loss: 0.5181 - val_accuracy: 0.8454\n",
            "Epoch 679/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1464 - accuracy: 0.9480 - val_loss: 0.4974 - val_accuracy: 0.8449\n",
            "Epoch 680/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1482 - accuracy: 0.9503 - val_loss: 0.5059 - val_accuracy: 0.8454\n",
            "Epoch 681/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1369 - accuracy: 0.9537 - val_loss: 0.5124 - val_accuracy: 0.8443\n",
            "Epoch 682/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1405 - accuracy: 0.9486 - val_loss: 0.5042 - val_accuracy: 0.8426\n",
            "Epoch 683/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1413 - accuracy: 0.9505 - val_loss: 0.5084 - val_accuracy: 0.8408\n",
            "Epoch 684/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1474 - accuracy: 0.9486 - val_loss: 0.4908 - val_accuracy: 0.8478\n",
            "Epoch 685/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1375 - accuracy: 0.9517 - val_loss: 0.4982 - val_accuracy: 0.8391\n",
            "Epoch 686/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1297 - accuracy: 0.9576 - val_loss: 0.4962 - val_accuracy: 0.8356\n",
            "Epoch 687/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1377 - accuracy: 0.9520 - val_loss: 0.5049 - val_accuracy: 0.8483\n",
            "Epoch 688/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1375 - accuracy: 0.9520 - val_loss: 0.4857 - val_accuracy: 0.8449\n",
            "Epoch 689/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1449 - accuracy: 0.9514 - val_loss: 0.4997 - val_accuracy: 0.8460\n",
            "Epoch 690/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1353 - accuracy: 0.9545 - val_loss: 0.4998 - val_accuracy: 0.8518\n",
            "Epoch 691/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1372 - accuracy: 0.9542 - val_loss: 0.4808 - val_accuracy: 0.8431\n",
            "Epoch 692/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1430 - accuracy: 0.9525 - val_loss: 0.4818 - val_accuracy: 0.8449\n",
            "Epoch 693/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1344 - accuracy: 0.9511 - val_loss: 0.4822 - val_accuracy: 0.8495\n",
            "Epoch 694/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1363 - accuracy: 0.9511 - val_loss: 0.4963 - val_accuracy: 0.8454\n",
            "Epoch 695/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1488 - accuracy: 0.9494 - val_loss: 0.5169 - val_accuracy: 0.8362\n",
            "Epoch 696/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1347 - accuracy: 0.9497 - val_loss: 0.4985 - val_accuracy: 0.8460\n",
            "Epoch 697/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1276 - accuracy: 0.9537 - val_loss: 0.5079 - val_accuracy: 0.8368\n",
            "Epoch 698/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1420 - accuracy: 0.9534 - val_loss: 0.4952 - val_accuracy: 0.8374\n",
            "Epoch 699/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1330 - accuracy: 0.9528 - val_loss: 0.5027 - val_accuracy: 0.8351\n",
            "Epoch 700/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1397 - accuracy: 0.9497 - val_loss: 0.5171 - val_accuracy: 0.8478\n",
            "Epoch 701/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1381 - accuracy: 0.9474 - val_loss: 0.4981 - val_accuracy: 0.8379\n",
            "Epoch 702/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1324 - accuracy: 0.9557 - val_loss: 0.4937 - val_accuracy: 0.8339\n",
            "Epoch 703/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1410 - accuracy: 0.9466 - val_loss: 0.5055 - val_accuracy: 0.8460\n",
            "Epoch 704/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1281 - accuracy: 0.9582 - val_loss: 0.5110 - val_accuracy: 0.8460\n",
            "Epoch 705/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1273 - accuracy: 0.9548 - val_loss: 0.5154 - val_accuracy: 0.8397\n",
            "Epoch 706/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1362 - accuracy: 0.9500 - val_loss: 0.5078 - val_accuracy: 0.8466\n",
            "Epoch 707/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1413 - accuracy: 0.9514 - val_loss: 0.5137 - val_accuracy: 0.8403\n",
            "Epoch 708/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1292 - accuracy: 0.9565 - val_loss: 0.4846 - val_accuracy: 0.8524\n",
            "Epoch 709/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1312 - accuracy: 0.9551 - val_loss: 0.5127 - val_accuracy: 0.8483\n",
            "Epoch 710/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1412 - accuracy: 0.9522 - val_loss: 0.4971 - val_accuracy: 0.8478\n",
            "Epoch 711/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1334 - accuracy: 0.9542 - val_loss: 0.4831 - val_accuracy: 0.8495\n",
            "Epoch 712/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1365 - accuracy: 0.9562 - val_loss: 0.5021 - val_accuracy: 0.8495\n",
            "Epoch 713/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1383 - accuracy: 0.9525 - val_loss: 0.4920 - val_accuracy: 0.8449\n",
            "Epoch 714/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1295 - accuracy: 0.9500 - val_loss: 0.4986 - val_accuracy: 0.8478\n",
            "Epoch 715/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1272 - accuracy: 0.9534 - val_loss: 0.4947 - val_accuracy: 0.8518\n",
            "Epoch 716/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1328 - accuracy: 0.9534 - val_loss: 0.4856 - val_accuracy: 0.8414\n",
            "Epoch 717/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1300 - accuracy: 0.9545 - val_loss: 0.5008 - val_accuracy: 0.8374\n",
            "Epoch 718/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1527 - accuracy: 0.9483 - val_loss: 0.4964 - val_accuracy: 0.8483\n",
            "Epoch 719/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1309 - accuracy: 0.9540 - val_loss: 0.4833 - val_accuracy: 0.8478\n",
            "Epoch 720/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1230 - accuracy: 0.9568 - val_loss: 0.5192 - val_accuracy: 0.8426\n",
            "Epoch 721/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1365 - accuracy: 0.9474 - val_loss: 0.4975 - val_accuracy: 0.8431\n",
            "Epoch 722/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1349 - accuracy: 0.9531 - val_loss: 0.5195 - val_accuracy: 0.8466\n",
            "Epoch 723/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1244 - accuracy: 0.9554 - val_loss: 0.4999 - val_accuracy: 0.8414\n",
            "Epoch 724/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1287 - accuracy: 0.9565 - val_loss: 0.4872 - val_accuracy: 0.8420\n",
            "Epoch 725/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1333 - accuracy: 0.9565 - val_loss: 0.4847 - val_accuracy: 0.8483\n",
            "Epoch 726/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1333 - accuracy: 0.9545 - val_loss: 0.5005 - val_accuracy: 0.8443\n",
            "Epoch 727/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1365 - accuracy: 0.9505 - val_loss: 0.4824 - val_accuracy: 0.8420\n",
            "Epoch 728/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1267 - accuracy: 0.9599 - val_loss: 0.5208 - val_accuracy: 0.8460\n",
            "Epoch 729/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1286 - accuracy: 0.9531 - val_loss: 0.5142 - val_accuracy: 0.8443\n",
            "Epoch 730/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1349 - accuracy: 0.9511 - val_loss: 0.5252 - val_accuracy: 0.8391\n",
            "Epoch 731/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1189 - accuracy: 0.9616 - val_loss: 0.5271 - val_accuracy: 0.8431\n",
            "Epoch 732/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1363 - accuracy: 0.9542 - val_loss: 0.5141 - val_accuracy: 0.8460\n",
            "Epoch 733/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1316 - accuracy: 0.9608 - val_loss: 0.5327 - val_accuracy: 0.8351\n",
            "Epoch 734/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1254 - accuracy: 0.9568 - val_loss: 0.4860 - val_accuracy: 0.8501\n",
            "Epoch 735/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1279 - accuracy: 0.9574 - val_loss: 0.5297 - val_accuracy: 0.8426\n",
            "Epoch 736/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1264 - accuracy: 0.9582 - val_loss: 0.5072 - val_accuracy: 0.8449\n",
            "Epoch 737/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1374 - accuracy: 0.9522 - val_loss: 0.4917 - val_accuracy: 0.8431\n",
            "Epoch 738/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1195 - accuracy: 0.9579 - val_loss: 0.4899 - val_accuracy: 0.8518\n",
            "Epoch 739/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1250 - accuracy: 0.9542 - val_loss: 0.5037 - val_accuracy: 0.8449\n",
            "Epoch 740/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1297 - accuracy: 0.9542 - val_loss: 0.4920 - val_accuracy: 0.8460\n",
            "Epoch 741/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1240 - accuracy: 0.9588 - val_loss: 0.5093 - val_accuracy: 0.8483\n",
            "Epoch 742/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1228 - accuracy: 0.9528 - val_loss: 0.5076 - val_accuracy: 0.8483\n",
            "Epoch 743/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1277 - accuracy: 0.9585 - val_loss: 0.5060 - val_accuracy: 0.8541\n",
            "Epoch 744/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1266 - accuracy: 0.9579 - val_loss: 0.5134 - val_accuracy: 0.8431\n",
            "Epoch 745/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1280 - accuracy: 0.9554 - val_loss: 0.5002 - val_accuracy: 0.8552\n",
            "Epoch 746/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1197 - accuracy: 0.9585 - val_loss: 0.4964 - val_accuracy: 0.8478\n",
            "Epoch 747/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1276 - accuracy: 0.9525 - val_loss: 0.5175 - val_accuracy: 0.8437\n",
            "Epoch 748/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1236 - accuracy: 0.9565 - val_loss: 0.5145 - val_accuracy: 0.8466\n",
            "Epoch 749/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1260 - accuracy: 0.9565 - val_loss: 0.5051 - val_accuracy: 0.8489\n",
            "Epoch 750/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1291 - accuracy: 0.9565 - val_loss: 0.5199 - val_accuracy: 0.8385\n",
            "Epoch 751/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1209 - accuracy: 0.9596 - val_loss: 0.4993 - val_accuracy: 0.8466\n",
            "Epoch 752/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1255 - accuracy: 0.9545 - val_loss: 0.5045 - val_accuracy: 0.8460\n",
            "Epoch 753/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1267 - accuracy: 0.9559 - val_loss: 0.4987 - val_accuracy: 0.8495\n",
            "Epoch 754/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1192 - accuracy: 0.9591 - val_loss: 0.5049 - val_accuracy: 0.8483\n",
            "Epoch 755/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1186 - accuracy: 0.9594 - val_loss: 0.5020 - val_accuracy: 0.8408\n",
            "Epoch 756/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1164 - accuracy: 0.9611 - val_loss: 0.5183 - val_accuracy: 0.8420\n",
            "Epoch 757/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1203 - accuracy: 0.9616 - val_loss: 0.5114 - val_accuracy: 0.8472\n",
            "Epoch 758/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1214 - accuracy: 0.9599 - val_loss: 0.5108 - val_accuracy: 0.8501\n",
            "Epoch 759/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1096 - accuracy: 0.9616 - val_loss: 0.4996 - val_accuracy: 0.8512\n",
            "Epoch 760/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1226 - accuracy: 0.9585 - val_loss: 0.4962 - val_accuracy: 0.8535\n",
            "Epoch 761/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1188 - accuracy: 0.9574 - val_loss: 0.5181 - val_accuracy: 0.8472\n",
            "Epoch 762/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1216 - accuracy: 0.9554 - val_loss: 0.5240 - val_accuracy: 0.8483\n",
            "Epoch 763/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1249 - accuracy: 0.9576 - val_loss: 0.5343 - val_accuracy: 0.8478\n",
            "Epoch 764/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1104 - accuracy: 0.9625 - val_loss: 0.5175 - val_accuracy: 0.8431\n",
            "Epoch 765/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1196 - accuracy: 0.9605 - val_loss: 0.5363 - val_accuracy: 0.8420\n",
            "Epoch 766/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1246 - accuracy: 0.9531 - val_loss: 0.4943 - val_accuracy: 0.8345\n",
            "Epoch 767/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1156 - accuracy: 0.9591 - val_loss: 0.4880 - val_accuracy: 0.8478\n",
            "Epoch 768/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1266 - accuracy: 0.9554 - val_loss: 0.5139 - val_accuracy: 0.8443\n",
            "Epoch 769/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1292 - accuracy: 0.9534 - val_loss: 0.5029 - val_accuracy: 0.8420\n",
            "Epoch 770/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1189 - accuracy: 0.9588 - val_loss: 0.5127 - val_accuracy: 0.8483\n",
            "Epoch 771/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1112 - accuracy: 0.9596 - val_loss: 0.5045 - val_accuracy: 0.8535\n",
            "Epoch 772/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1126 - accuracy: 0.9608 - val_loss: 0.5378 - val_accuracy: 0.8460\n",
            "Epoch 773/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1160 - accuracy: 0.9576 - val_loss: 0.5020 - val_accuracy: 0.8391\n",
            "Epoch 774/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1155 - accuracy: 0.9599 - val_loss: 0.5083 - val_accuracy: 0.8478\n",
            "Epoch 775/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1331 - accuracy: 0.9568 - val_loss: 0.5168 - val_accuracy: 0.8478\n",
            "Epoch 776/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1205 - accuracy: 0.9611 - val_loss: 0.5340 - val_accuracy: 0.8506\n",
            "Epoch 777/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1201 - accuracy: 0.9579 - val_loss: 0.5091 - val_accuracy: 0.8506\n",
            "Epoch 778/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1175 - accuracy: 0.9611 - val_loss: 0.5092 - val_accuracy: 0.8408\n",
            "Epoch 779/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1259 - accuracy: 0.9565 - val_loss: 0.4938 - val_accuracy: 0.8524\n",
            "Epoch 780/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1150 - accuracy: 0.9608 - val_loss: 0.5033 - val_accuracy: 0.8403\n",
            "Epoch 781/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1106 - accuracy: 0.9633 - val_loss: 0.4960 - val_accuracy: 0.8512\n",
            "Epoch 782/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1199 - accuracy: 0.9591 - val_loss: 0.5377 - val_accuracy: 0.8420\n",
            "Epoch 783/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1092 - accuracy: 0.9645 - val_loss: 0.5066 - val_accuracy: 0.8489\n",
            "Epoch 784/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1225 - accuracy: 0.9588 - val_loss: 0.5246 - val_accuracy: 0.8460\n",
            "Epoch 785/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1160 - accuracy: 0.9636 - val_loss: 0.5284 - val_accuracy: 0.8489\n",
            "Epoch 786/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1158 - accuracy: 0.9588 - val_loss: 0.5243 - val_accuracy: 0.8460\n",
            "Epoch 787/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1118 - accuracy: 0.9588 - val_loss: 0.4969 - val_accuracy: 0.8466\n",
            "Epoch 788/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1210 - accuracy: 0.9585 - val_loss: 0.5361 - val_accuracy: 0.8385\n",
            "Epoch 789/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1193 - accuracy: 0.9616 - val_loss: 0.5446 - val_accuracy: 0.8443\n",
            "Epoch 790/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1124 - accuracy: 0.9596 - val_loss: 0.5268 - val_accuracy: 0.8431\n",
            "Epoch 791/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1244 - accuracy: 0.9540 - val_loss: 0.5273 - val_accuracy: 0.8403\n",
            "Epoch 792/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1168 - accuracy: 0.9613 - val_loss: 0.5422 - val_accuracy: 0.8362\n",
            "Epoch 793/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1224 - accuracy: 0.9568 - val_loss: 0.5382 - val_accuracy: 0.8449\n",
            "Epoch 794/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1238 - accuracy: 0.9551 - val_loss: 0.5048 - val_accuracy: 0.8449\n",
            "Epoch 795/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1141 - accuracy: 0.9622 - val_loss: 0.5390 - val_accuracy: 0.8501\n",
            "Epoch 796/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1125 - accuracy: 0.9613 - val_loss: 0.5009 - val_accuracy: 0.8466\n",
            "Epoch 797/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1171 - accuracy: 0.9596 - val_loss: 0.5430 - val_accuracy: 0.8426\n",
            "Epoch 798/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1118 - accuracy: 0.9613 - val_loss: 0.5203 - val_accuracy: 0.8443\n",
            "Epoch 799/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1171 - accuracy: 0.9588 - val_loss: 0.5085 - val_accuracy: 0.8466\n",
            "Epoch 800/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1141 - accuracy: 0.9611 - val_loss: 0.5078 - val_accuracy: 0.8437\n",
            "Epoch 801/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1185 - accuracy: 0.9611 - val_loss: 0.5144 - val_accuracy: 0.8512\n",
            "Epoch 802/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1170 - accuracy: 0.9605 - val_loss: 0.5050 - val_accuracy: 0.8501\n",
            "Epoch 803/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1096 - accuracy: 0.9605 - val_loss: 0.5180 - val_accuracy: 0.8478\n",
            "Epoch 804/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1190 - accuracy: 0.9585 - val_loss: 0.5207 - val_accuracy: 0.8466\n",
            "Epoch 805/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1216 - accuracy: 0.9619 - val_loss: 0.5107 - val_accuracy: 0.8449\n",
            "Epoch 806/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1294 - accuracy: 0.9537 - val_loss: 0.5086 - val_accuracy: 0.8489\n",
            "Epoch 807/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1102 - accuracy: 0.9642 - val_loss: 0.5452 - val_accuracy: 0.8472\n",
            "Epoch 808/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1181 - accuracy: 0.9562 - val_loss: 0.4952 - val_accuracy: 0.8426\n",
            "Epoch 809/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1082 - accuracy: 0.9628 - val_loss: 0.5229 - val_accuracy: 0.8506\n",
            "Epoch 810/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1285 - accuracy: 0.9545 - val_loss: 0.5353 - val_accuracy: 0.8512\n",
            "Epoch 811/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1126 - accuracy: 0.9596 - val_loss: 0.5049 - val_accuracy: 0.8449\n",
            "Epoch 812/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1119 - accuracy: 0.9599 - val_loss: 0.5109 - val_accuracy: 0.8449\n",
            "Epoch 813/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1000 - accuracy: 0.9682 - val_loss: 0.5065 - val_accuracy: 0.8524\n",
            "Epoch 814/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1020 - accuracy: 0.9639 - val_loss: 0.5303 - val_accuracy: 0.8466\n",
            "Epoch 815/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1004 - accuracy: 0.9659 - val_loss: 0.5086 - val_accuracy: 0.8478\n",
            "Epoch 816/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1174 - accuracy: 0.9605 - val_loss: 0.5241 - val_accuracy: 0.8483\n",
            "Epoch 817/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1162 - accuracy: 0.9613 - val_loss: 0.5155 - val_accuracy: 0.8414\n",
            "Epoch 818/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1194 - accuracy: 0.9599 - val_loss: 0.5094 - val_accuracy: 0.8512\n",
            "Epoch 819/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1058 - accuracy: 0.9630 - val_loss: 0.5233 - val_accuracy: 0.8472\n",
            "Epoch 820/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1084 - accuracy: 0.9648 - val_loss: 0.5260 - val_accuracy: 0.8460\n",
            "Epoch 821/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1074 - accuracy: 0.9608 - val_loss: 0.5094 - val_accuracy: 0.8495\n",
            "Epoch 822/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1117 - accuracy: 0.9605 - val_loss: 0.5133 - val_accuracy: 0.8483\n",
            "Epoch 823/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1073 - accuracy: 0.9630 - val_loss: 0.5082 - val_accuracy: 0.8460\n",
            "Epoch 824/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1074 - accuracy: 0.9628 - val_loss: 0.5348 - val_accuracy: 0.8408\n",
            "Epoch 825/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1060 - accuracy: 0.9648 - val_loss: 0.5289 - val_accuracy: 0.8449\n",
            "Epoch 826/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1123 - accuracy: 0.9608 - val_loss: 0.5170 - val_accuracy: 0.8466\n",
            "Epoch 827/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1128 - accuracy: 0.9630 - val_loss: 0.5413 - val_accuracy: 0.8483\n",
            "Epoch 828/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1023 - accuracy: 0.9642 - val_loss: 0.5388 - val_accuracy: 0.8397\n",
            "Epoch 829/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1112 - accuracy: 0.9582 - val_loss: 0.5090 - val_accuracy: 0.8408\n",
            "Epoch 830/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1079 - accuracy: 0.9633 - val_loss: 0.5160 - val_accuracy: 0.8472\n",
            "Epoch 831/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1115 - accuracy: 0.9596 - val_loss: 0.4957 - val_accuracy: 0.8501\n",
            "Epoch 832/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0966 - accuracy: 0.9690 - val_loss: 0.5167 - val_accuracy: 0.8454\n",
            "Epoch 833/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1192 - accuracy: 0.9568 - val_loss: 0.5209 - val_accuracy: 0.8460\n",
            "Epoch 834/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1131 - accuracy: 0.9576 - val_loss: 0.5398 - val_accuracy: 0.8472\n",
            "Epoch 835/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1049 - accuracy: 0.9628 - val_loss: 0.5296 - val_accuracy: 0.8460\n",
            "Epoch 836/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1027 - accuracy: 0.9670 - val_loss: 0.5637 - val_accuracy: 0.8351\n",
            "Epoch 837/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1043 - accuracy: 0.9659 - val_loss: 0.5477 - val_accuracy: 0.8368\n",
            "Epoch 838/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1066 - accuracy: 0.9619 - val_loss: 0.5477 - val_accuracy: 0.8478\n",
            "Epoch 839/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1111 - accuracy: 0.9585 - val_loss: 0.5423 - val_accuracy: 0.8483\n",
            "Epoch 840/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1102 - accuracy: 0.9636 - val_loss: 0.5140 - val_accuracy: 0.8478\n",
            "Epoch 841/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.0972 - accuracy: 0.9665 - val_loss: 0.5403 - val_accuracy: 0.8431\n",
            "Epoch 842/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1063 - accuracy: 0.9642 - val_loss: 0.5125 - val_accuracy: 0.8443\n",
            "Epoch 843/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1059 - accuracy: 0.9625 - val_loss: 0.5189 - val_accuracy: 0.8454\n",
            "Epoch 844/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1023 - accuracy: 0.9636 - val_loss: 0.5270 - val_accuracy: 0.8506\n",
            "Epoch 845/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0966 - accuracy: 0.9679 - val_loss: 0.5125 - val_accuracy: 0.8454\n",
            "Epoch 846/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0947 - accuracy: 0.9670 - val_loss: 0.5111 - val_accuracy: 0.8408\n",
            "Epoch 847/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1109 - accuracy: 0.9613 - val_loss: 0.5343 - val_accuracy: 0.8443\n",
            "Epoch 848/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1036 - accuracy: 0.9630 - val_loss: 0.5325 - val_accuracy: 0.8547\n",
            "Epoch 849/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1063 - accuracy: 0.9628 - val_loss: 0.5209 - val_accuracy: 0.8426\n",
            "Epoch 850/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1069 - accuracy: 0.9619 - val_loss: 0.5185 - val_accuracy: 0.8535\n",
            "Epoch 851/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1067 - accuracy: 0.9619 - val_loss: 0.5337 - val_accuracy: 0.8478\n",
            "Epoch 852/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1044 - accuracy: 0.9667 - val_loss: 0.5329 - val_accuracy: 0.8472\n",
            "Epoch 853/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1049 - accuracy: 0.9633 - val_loss: 0.5539 - val_accuracy: 0.8478\n",
            "Epoch 854/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1083 - accuracy: 0.9625 - val_loss: 0.5364 - val_accuracy: 0.8454\n",
            "Epoch 855/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1062 - accuracy: 0.9639 - val_loss: 0.5236 - val_accuracy: 0.8460\n",
            "Epoch 856/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1029 - accuracy: 0.9670 - val_loss: 0.5239 - val_accuracy: 0.8495\n",
            "Epoch 857/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1030 - accuracy: 0.9633 - val_loss: 0.5407 - val_accuracy: 0.8495\n",
            "Epoch 858/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1037 - accuracy: 0.9622 - val_loss: 0.5273 - val_accuracy: 0.8449\n",
            "Epoch 859/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1173 - accuracy: 0.9599 - val_loss: 0.5493 - val_accuracy: 0.8483\n",
            "Epoch 860/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1085 - accuracy: 0.9613 - val_loss: 0.5141 - val_accuracy: 0.8379\n",
            "Epoch 861/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1069 - accuracy: 0.9636 - val_loss: 0.5214 - val_accuracy: 0.8466\n",
            "Epoch 862/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0982 - accuracy: 0.9645 - val_loss: 0.5129 - val_accuracy: 0.8529\n",
            "Epoch 863/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.0979 - accuracy: 0.9673 - val_loss: 0.5215 - val_accuracy: 0.8454\n",
            "Epoch 864/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1015 - accuracy: 0.9648 - val_loss: 0.5533 - val_accuracy: 0.8495\n",
            "Epoch 865/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1017 - accuracy: 0.9650 - val_loss: 0.5473 - val_accuracy: 0.8478\n",
            "Epoch 866/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1000 - accuracy: 0.9670 - val_loss: 0.5539 - val_accuracy: 0.8431\n",
            "Epoch 867/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1123 - accuracy: 0.9619 - val_loss: 0.5351 - val_accuracy: 0.8483\n",
            "Epoch 868/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1010 - accuracy: 0.9656 - val_loss: 0.5638 - val_accuracy: 0.8431\n",
            "Epoch 869/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.0999 - accuracy: 0.9682 - val_loss: 0.5558 - val_accuracy: 0.8472\n",
            "Epoch 870/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.0874 - accuracy: 0.9690 - val_loss: 0.5222 - val_accuracy: 0.8443\n",
            "Epoch 871/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1022 - accuracy: 0.9667 - val_loss: 0.5185 - val_accuracy: 0.8460\n",
            "Epoch 872/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0999 - accuracy: 0.9650 - val_loss: 0.5223 - val_accuracy: 0.8483\n",
            "Epoch 873/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0967 - accuracy: 0.9679 - val_loss: 0.5490 - val_accuracy: 0.8443\n",
            "Epoch 874/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1031 - accuracy: 0.9653 - val_loss: 0.5545 - val_accuracy: 0.8454\n",
            "Epoch 875/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0996 - accuracy: 0.9676 - val_loss: 0.5314 - val_accuracy: 0.8512\n",
            "Epoch 876/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1076 - accuracy: 0.9628 - val_loss: 0.5421 - val_accuracy: 0.8495\n",
            "Epoch 877/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1039 - accuracy: 0.9648 - val_loss: 0.5437 - val_accuracy: 0.8541\n",
            "Epoch 878/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1014 - accuracy: 0.9659 - val_loss: 0.5606 - val_accuracy: 0.8489\n",
            "Epoch 879/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.0991 - accuracy: 0.9630 - val_loss: 0.5354 - val_accuracy: 0.8466\n",
            "Epoch 880/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1046 - accuracy: 0.9648 - val_loss: 0.5423 - val_accuracy: 0.8483\n",
            "Epoch 881/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0994 - accuracy: 0.9659 - val_loss: 0.5726 - val_accuracy: 0.8472\n",
            "Epoch 882/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0981 - accuracy: 0.9667 - val_loss: 0.5179 - val_accuracy: 0.8391\n",
            "Epoch 883/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0959 - accuracy: 0.9670 - val_loss: 0.5351 - val_accuracy: 0.8483\n",
            "Epoch 884/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1055 - accuracy: 0.9645 - val_loss: 0.5185 - val_accuracy: 0.8454\n",
            "Epoch 885/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.0963 - accuracy: 0.9665 - val_loss: 0.5422 - val_accuracy: 0.8460\n",
            "Epoch 886/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1060 - accuracy: 0.9625 - val_loss: 0.5491 - val_accuracy: 0.8391\n",
            "Epoch 887/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1079 - accuracy: 0.9645 - val_loss: 0.5176 - val_accuracy: 0.8524\n",
            "Epoch 888/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1064 - accuracy: 0.9619 - val_loss: 0.5624 - val_accuracy: 0.8454\n",
            "Epoch 889/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0986 - accuracy: 0.9659 - val_loss: 0.5558 - val_accuracy: 0.8460\n",
            "Epoch 890/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0979 - accuracy: 0.9653 - val_loss: 0.5697 - val_accuracy: 0.8403\n",
            "Epoch 891/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.0977 - accuracy: 0.9645 - val_loss: 0.5438 - val_accuracy: 0.8483\n",
            "Epoch 892/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.0920 - accuracy: 0.9679 - val_loss: 0.5551 - val_accuracy: 0.8437\n",
            "Epoch 893/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1025 - accuracy: 0.9648 - val_loss: 0.5600 - val_accuracy: 0.8420\n",
            "Epoch 894/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.0964 - accuracy: 0.9673 - val_loss: 0.5656 - val_accuracy: 0.8483\n",
            "Epoch 895/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1008 - accuracy: 0.9676 - val_loss: 0.5704 - val_accuracy: 0.8391\n",
            "Epoch 896/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1070 - accuracy: 0.9642 - val_loss: 0.5409 - val_accuracy: 0.8460\n",
            "Epoch 897/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.0982 - accuracy: 0.9702 - val_loss: 0.5430 - val_accuracy: 0.8443\n",
            "Epoch 898/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.0972 - accuracy: 0.9662 - val_loss: 0.5618 - val_accuracy: 0.8403\n",
            "Epoch 899/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.0915 - accuracy: 0.9673 - val_loss: 0.5703 - val_accuracy: 0.8478\n",
            "Epoch 900/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1013 - accuracy: 0.9645 - val_loss: 0.5041 - val_accuracy: 0.8501\n",
            "Epoch 901/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.0907 - accuracy: 0.9702 - val_loss: 0.5511 - val_accuracy: 0.8489\n",
            "Epoch 902/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1008 - accuracy: 0.9684 - val_loss: 0.5377 - val_accuracy: 0.8431\n",
            "Epoch 903/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.0942 - accuracy: 0.9702 - val_loss: 0.5481 - val_accuracy: 0.8466\n",
            "Epoch 904/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.0983 - accuracy: 0.9665 - val_loss: 0.5264 - val_accuracy: 0.8512\n",
            "Epoch 905/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1017 - accuracy: 0.9684 - val_loss: 0.5467 - val_accuracy: 0.8431\n",
            "Epoch 906/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0955 - accuracy: 0.9639 - val_loss: 0.5476 - val_accuracy: 0.8495\n",
            "Epoch 907/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1035 - accuracy: 0.9611 - val_loss: 0.5930 - val_accuracy: 0.8391\n",
            "Epoch 908/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.0920 - accuracy: 0.9679 - val_loss: 0.5417 - val_accuracy: 0.8501\n",
            "Epoch 909/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0879 - accuracy: 0.9702 - val_loss: 0.5592 - val_accuracy: 0.8431\n",
            "Epoch 910/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0960 - accuracy: 0.9676 - val_loss: 0.5547 - val_accuracy: 0.8472\n",
            "Epoch 911/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0897 - accuracy: 0.9716 - val_loss: 0.5588 - val_accuracy: 0.8460\n",
            "Epoch 912/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0810 - accuracy: 0.9713 - val_loss: 0.5773 - val_accuracy: 0.8443\n",
            "Epoch 913/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0913 - accuracy: 0.9650 - val_loss: 0.5703 - val_accuracy: 0.8449\n",
            "Epoch 914/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0987 - accuracy: 0.9684 - val_loss: 0.5637 - val_accuracy: 0.8443\n",
            "Epoch 915/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0902 - accuracy: 0.9684 - val_loss: 0.5167 - val_accuracy: 0.8524\n",
            "Epoch 916/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.0931 - accuracy: 0.9645 - val_loss: 0.5562 - val_accuracy: 0.8449\n",
            "Epoch 917/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1014 - accuracy: 0.9622 - val_loss: 0.5470 - val_accuracy: 0.8483\n",
            "Epoch 918/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0960 - accuracy: 0.9687 - val_loss: 0.5673 - val_accuracy: 0.8483\n",
            "Epoch 919/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0881 - accuracy: 0.9693 - val_loss: 0.5459 - val_accuracy: 0.8489\n",
            "Epoch 920/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0858 - accuracy: 0.9730 - val_loss: 0.5527 - val_accuracy: 0.8460\n",
            "Epoch 921/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0940 - accuracy: 0.9667 - val_loss: 0.5670 - val_accuracy: 0.8466\n",
            "Epoch 922/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1024 - accuracy: 0.9665 - val_loss: 0.5727 - val_accuracy: 0.8362\n",
            "Epoch 923/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.0935 - accuracy: 0.9667 - val_loss: 0.5359 - val_accuracy: 0.8483\n",
            "Epoch 924/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.1001 - accuracy: 0.9682 - val_loss: 0.5311 - val_accuracy: 0.8431\n",
            "Epoch 925/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.0944 - accuracy: 0.9702 - val_loss: 0.5468 - val_accuracy: 0.8506\n",
            "Epoch 926/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0878 - accuracy: 0.9704 - val_loss: 0.5430 - val_accuracy: 0.8501\n",
            "Epoch 927/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.0836 - accuracy: 0.9719 - val_loss: 0.5566 - val_accuracy: 0.8420\n",
            "Epoch 928/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0919 - accuracy: 0.9702 - val_loss: 0.5394 - val_accuracy: 0.8483\n",
            "Epoch 929/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.0919 - accuracy: 0.9667 - val_loss: 0.5318 - val_accuracy: 0.8466\n",
            "Epoch 930/1000\n",
            "220/220 [==============================] - 2s 8ms/step - loss: 0.1000 - accuracy: 0.9670 - val_loss: 0.5550 - val_accuracy: 0.8506\n",
            "Epoch 931/1000\n",
            "220/220 [==============================] - 2s 7ms/step - loss: 0.1006 - accuracy: 0.9625 - val_loss: 0.5417 - val_accuracy: 0.8489\n",
            "Epoch 932/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.0829 - accuracy: 0.9682 - val_loss: 0.5808 - val_accuracy: 0.8483\n",
            "Epoch 933/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.0964 - accuracy: 0.9676 - val_loss: 0.5659 - val_accuracy: 0.8431\n",
            "Epoch 934/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0888 - accuracy: 0.9702 - val_loss: 0.5378 - val_accuracy: 0.8472\n",
            "Epoch 935/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.0847 - accuracy: 0.9699 - val_loss: 0.5539 - val_accuracy: 0.8460\n",
            "Epoch 936/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.1020 - accuracy: 0.9633 - val_loss: 0.5602 - val_accuracy: 0.8431\n",
            "Epoch 937/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.0909 - accuracy: 0.9673 - val_loss: 0.5754 - val_accuracy: 0.8437\n",
            "Epoch 938/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0935 - accuracy: 0.9684 - val_loss: 0.5413 - val_accuracy: 0.8506\n",
            "Epoch 939/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.0772 - accuracy: 0.9741 - val_loss: 0.5947 - val_accuracy: 0.8426\n",
            "Epoch 940/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.0900 - accuracy: 0.9679 - val_loss: 0.5558 - val_accuracy: 0.8495\n",
            "Epoch 941/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.0862 - accuracy: 0.9687 - val_loss: 0.5631 - val_accuracy: 0.8460\n",
            "Epoch 942/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0853 - accuracy: 0.9699 - val_loss: 0.5594 - val_accuracy: 0.8379\n",
            "Epoch 943/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0922 - accuracy: 0.9682 - val_loss: 0.5157 - val_accuracy: 0.8483\n",
            "Epoch 944/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0954 - accuracy: 0.9673 - val_loss: 0.5497 - val_accuracy: 0.8483\n",
            "Epoch 945/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.0934 - accuracy: 0.9667 - val_loss: 0.5799 - val_accuracy: 0.8495\n",
            "Epoch 946/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.0817 - accuracy: 0.9707 - val_loss: 0.5390 - val_accuracy: 0.8466\n",
            "Epoch 947/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0896 - accuracy: 0.9716 - val_loss: 0.5605 - val_accuracy: 0.8518\n",
            "Epoch 948/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.0811 - accuracy: 0.9702 - val_loss: 0.5652 - val_accuracy: 0.8483\n",
            "Epoch 949/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.0939 - accuracy: 0.9648 - val_loss: 0.5660 - val_accuracy: 0.8454\n",
            "Epoch 950/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.0933 - accuracy: 0.9687 - val_loss: 0.5360 - val_accuracy: 0.8541\n",
            "Epoch 951/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0983 - accuracy: 0.9696 - val_loss: 0.5392 - val_accuracy: 0.8478\n",
            "Epoch 952/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0837 - accuracy: 0.9727 - val_loss: 0.5341 - val_accuracy: 0.8483\n",
            "Epoch 953/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0866 - accuracy: 0.9710 - val_loss: 0.5460 - val_accuracy: 0.8512\n",
            "Epoch 954/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.0856 - accuracy: 0.9704 - val_loss: 0.5499 - val_accuracy: 0.8501\n",
            "Epoch 955/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0822 - accuracy: 0.9724 - val_loss: 0.5606 - val_accuracy: 0.8454\n",
            "Epoch 956/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0921 - accuracy: 0.9702 - val_loss: 0.5608 - val_accuracy: 0.8460\n",
            "Epoch 957/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.0905 - accuracy: 0.9662 - val_loss: 0.5592 - val_accuracy: 0.8472\n",
            "Epoch 958/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0881 - accuracy: 0.9696 - val_loss: 0.5633 - val_accuracy: 0.8420\n",
            "Epoch 959/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0872 - accuracy: 0.9645 - val_loss: 0.5757 - val_accuracy: 0.8472\n",
            "Epoch 960/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.0850 - accuracy: 0.9721 - val_loss: 0.5498 - val_accuracy: 0.8512\n",
            "Epoch 961/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0912 - accuracy: 0.9684 - val_loss: 0.5673 - val_accuracy: 0.8449\n",
            "Epoch 962/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0928 - accuracy: 0.9673 - val_loss: 0.5399 - val_accuracy: 0.8443\n",
            "Epoch 963/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.0923 - accuracy: 0.9642 - val_loss: 0.5538 - val_accuracy: 0.8501\n",
            "Epoch 964/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.0901 - accuracy: 0.9676 - val_loss: 0.5550 - val_accuracy: 0.8535\n",
            "Epoch 965/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.0885 - accuracy: 0.9710 - val_loss: 0.6068 - val_accuracy: 0.8426\n",
            "Epoch 966/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.0983 - accuracy: 0.9665 - val_loss: 0.5704 - val_accuracy: 0.8437\n",
            "Epoch 967/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0743 - accuracy: 0.9747 - val_loss: 0.5451 - val_accuracy: 0.8449\n",
            "Epoch 968/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.0797 - accuracy: 0.9744 - val_loss: 0.5432 - val_accuracy: 0.8524\n",
            "Epoch 969/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.0833 - accuracy: 0.9696 - val_loss: 0.5934 - val_accuracy: 0.8454\n",
            "Epoch 970/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.0815 - accuracy: 0.9744 - val_loss: 0.5707 - val_accuracy: 0.8558\n",
            "Epoch 971/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.0814 - accuracy: 0.9704 - val_loss: 0.5533 - val_accuracy: 0.8478\n",
            "Epoch 972/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0876 - accuracy: 0.9702 - val_loss: 0.5782 - val_accuracy: 0.8524\n",
            "Epoch 973/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0792 - accuracy: 0.9733 - val_loss: 0.5595 - val_accuracy: 0.8524\n",
            "Epoch 974/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.0874 - accuracy: 0.9724 - val_loss: 0.5750 - val_accuracy: 0.8489\n",
            "Epoch 975/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.0819 - accuracy: 0.9758 - val_loss: 0.5691 - val_accuracy: 0.8460\n",
            "Epoch 976/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0944 - accuracy: 0.9687 - val_loss: 0.5729 - val_accuracy: 0.8478\n",
            "Epoch 977/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0882 - accuracy: 0.9702 - val_loss: 0.5762 - val_accuracy: 0.8483\n",
            "Epoch 978/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0948 - accuracy: 0.9679 - val_loss: 0.5790 - val_accuracy: 0.8414\n",
            "Epoch 979/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.0985 - accuracy: 0.9650 - val_loss: 0.5665 - val_accuracy: 0.8460\n",
            "Epoch 980/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.0820 - accuracy: 0.9690 - val_loss: 0.5602 - val_accuracy: 0.8489\n",
            "Epoch 981/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.0851 - accuracy: 0.9702 - val_loss: 0.5710 - val_accuracy: 0.8466\n",
            "Epoch 982/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0841 - accuracy: 0.9679 - val_loss: 0.5602 - val_accuracy: 0.8495\n",
            "Epoch 983/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.0919 - accuracy: 0.9676 - val_loss: 0.5677 - val_accuracy: 0.8489\n",
            "Epoch 984/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0863 - accuracy: 0.9719 - val_loss: 0.5757 - val_accuracy: 0.8489\n",
            "Epoch 985/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.0915 - accuracy: 0.9696 - val_loss: 0.5700 - val_accuracy: 0.8437\n",
            "Epoch 986/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.0789 - accuracy: 0.9727 - val_loss: 0.5665 - val_accuracy: 0.8495\n",
            "Epoch 987/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.0855 - accuracy: 0.9704 - val_loss: 0.5610 - val_accuracy: 0.8437\n",
            "Epoch 988/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0864 - accuracy: 0.9662 - val_loss: 0.5526 - val_accuracy: 0.8478\n",
            "Epoch 989/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0926 - accuracy: 0.9684 - val_loss: 0.5579 - val_accuracy: 0.8466\n",
            "Epoch 990/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0953 - accuracy: 0.9679 - val_loss: 0.5649 - val_accuracy: 0.8483\n",
            "Epoch 991/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0938 - accuracy: 0.9667 - val_loss: 0.5450 - val_accuracy: 0.8483\n",
            "Epoch 992/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0878 - accuracy: 0.9721 - val_loss: 0.5659 - val_accuracy: 0.8443\n",
            "Epoch 993/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0991 - accuracy: 0.9662 - val_loss: 0.5528 - val_accuracy: 0.8506\n",
            "Epoch 994/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.0961 - accuracy: 0.9684 - val_loss: 0.5740 - val_accuracy: 0.8466\n",
            "Epoch 995/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0863 - accuracy: 0.9713 - val_loss: 0.5476 - val_accuracy: 0.8495\n",
            "Epoch 996/1000\n",
            "220/220 [==============================] - 1s 5ms/step - loss: 0.0791 - accuracy: 0.9724 - val_loss: 0.5808 - val_accuracy: 0.8478\n",
            "Epoch 997/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.0909 - accuracy: 0.9690 - val_loss: 0.5713 - val_accuracy: 0.8414\n",
            "Epoch 998/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.0774 - accuracy: 0.9753 - val_loss: 0.5616 - val_accuracy: 0.8454\n",
            "Epoch 999/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.0920 - accuracy: 0.9696 - val_loss: 0.5703 - val_accuracy: 0.8426\n",
            "Epoch 1000/1000\n",
            "220/220 [==============================] - 1s 6ms/step - loss: 0.0711 - accuracy: 0.9736 - val_loss: 0.5627 - val_accuracy: 0.8483\n"
          ]
        }
      ],
      "source": [
        "cnnhistory=model.fit(x_traincnn, y_train, batch_size=16, epochs=1000, validation_data=(x_testcnn, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFytY6LDzgJ0"
      },
      "source": [
        "Let's plot the loss:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "TFz4ClZov9gZ",
        "outputId": "7fe56e27-cd61-430b-92fb-469c92c843a3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEWCAYAAABsY4yMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZwV1Z338c/vbr3TO8gmIKIRF0AQQc0YNSpqYsxoTDQ6SSYTzfNkJmYm40Qn2ziTZ5LJZJ8xJhqdLDrGPTFqXINr3ABRWWURsWmgF+h9u8t5/jjVTdMN2A3cXorv+/XqV/etW7fqVFf39576Vd1T5pxDRETCJzLcDRARkexQwIuIhJQCXkQkpBTwIiIhpYAXEQkpBbyISEgp4EUAM/ulmX1rgPNuMrMPHuhyRLJNAS8iElIKeBGRkFLAy6gRlEauNbM3zKzVzG41s3Fm9kczazazJ82stNf8F5rZSjNrMLOnzeyYXs/NMbNlwevuAnL7rOtDZrY8eO2fzeyE/Wzz58xsvZntMLMHzWxCMN3M7IdmVmNmTWb2ppkdFzx3vpmtCtq2xcz+cb9+YXLIU8DLaHMxcDZwFPBh4I/APwOV+L/nLwKY2VHAncCXguceAf5gZgkzSwC/A34DlAH3BMsleO0c4DbgaqAc+DnwoJnlDKahZnYm8G3gUmA88A7w2+Dpc4C/CLajOJinPnjuVuBq51wRcBzwp8GsV6SbAl5Gm/9yzm13zm0BngNeds695pzrAB4A5gTzfRx42Dn3hHMuCXwPyANOARYAceBHzrmkc+5e4NVe67gK+Llz7mXnXNo59yugM3jdYHwSuM05t8w51wlcDyw0s6lAEigC3geYc261c25r8LokMNPMxjjndjrnlg1yvSKAAl5Gn+29fm7fw+PC4OcJ+B4zAM65DPAuMDF4bovbfaS9d3r9PAX4clCeaTCzBmBy8LrB6NuGFnwvfaJz7k/AfwM3AjVmdrOZjQlmvRg4H3jHzJ4xs4WDXK8IoICX8KrGBzXga974kN4CbAUmBtO6Hd7r53eB/+ecK+n1le+cu/MA21CAL/lsAXDO/cQ5NxeYiS/VXBtMf9U59xFgLL6UdPcg1ysCKOAlvO4GLjCzs8wsDnwZX2b5M/AikAK+aGZxM/tLYH6v194CfN7MTg5OhhaY2QVmVjTINtwJfMbMZgf1+3/Hl5Q2mdlJwfLjQCvQAWSCcwSfNLPioLTUBGQO4PcghzAFvISSc24tcAXwX0Ad/oTsh51zXc65LuAvgU8DO/D1+vt7vXYJ8Dl8CWUnsD6Yd7BteBL4OnAf/qhhOvCJ4Okx+DeSnfgyTj3wn8FzVwKbzKwJ+Dy+li8yaKYbfoiIhJN68CIiIaWAFxEJKQW8iEhIKeBFREIqNtwN6K2iosJNnTp1uJshIjJqLF26tM45V7mn50ZUwE+dOpUlS5YMdzNEREYNM3tnb8+pRCMiElIKeBGRkFLAi4iE1Iiqwe9JMpmkqqqKjo6O4W5KVuXm5jJp0iTi8fhwN0VEQmLEB3xVVRVFRUVMnTqV3Qf/Cw/nHPX19VRVVTFt2rThbo6IhMSIL9F0dHRQXl4e2nAHMDPKy8tDf5QiIkMrqwFvZiVmdq+ZrTGz1ft744Iwh3u3Q2EbRWRoZbtE82PgUefcJcF9MPOzsZLtTR3kJ6IU5ap+LSLSLWs9eDMrxt9U+FaAYBzuhmysq7a5k5bOVDYWTUNDAz/96U8H/brzzz+fhoasbK6IyIBks0QzDagF/sfMXjOzXwS3LNuNmV1lZkvMbEltbe3+ry1Lw9rvLeBTqX2/oTzyyCOUlJRkp1EiIgOQzYCPAScCNznn5uBvS3Zd35mcczc75+Y55+ZVVu5xOIUBydZtS6677jo2bNjA7NmzOemkk3j/+9/PhRdeyMyZMwG46KKLmDt3Lsceeyw333xzz+umTp1KXV0dmzZt4phjjuFzn/scxx57LOeccw7t7e1Zaq2IyC7ZrMFXAVXOuZeDx/eyh4AfjBv+sJJV1U39prd1pYhFIiRig3+/mjlhDN/88LF7ff473/kOK1asYPny5Tz99NNccMEFrFixoudyxttuu42ysjLa29s56aSTuPjiiykvL99tGevWrePOO+/klltu4dJLL+W+++7jiiuuGHRbRUQGI2s9eOfcNuBdMzs6mHQWsCpb6xsq8+fP3+1a9Z/85CfMmjWLBQsW8O6777Ju3bp+r5k2bRqzZ88GYO7cuWzatGmomisih7BsX0Xzd8AdwRU0G4HPHMjC9tbTXlndSGl+ggkleQey+AEpKNh1GuHpp5/mySef5MUXXyQ/P58PfOADe7yWPScnp+fnaDSqEo2IDImsBrxzbjkwL5vryLaioiKam5v3+FxjYyOlpaXk5+ezZs0aXnrppSFunYjI3o34oQoGKlsnWcvLyzn11FM57rjjyMvLY9y4cT3PLVq0iJ/97Gccc8wxHH300SxYsCBLrRARGTxzLlvROHjz5s1zfW/4sXr1ao455ph9vm5ldSMl+QkmDkGJJpsGsq0iIr2Z2VLn3B4rJSN+LJqBMCx7XXgRkVEqFAHvKeFFRHoLTcAr3kVEdheagBcRkd2FI+A10q6ISD+hCHjlu4hIf6EIeGDIR5MciB/96Ee0tbUd5BaJiAxMeAI+SxTwIjJa6ZOs76H3cMFnn302Y8eO5e6776azs5OPfvSj3HDDDbS2tnLppZdSVVVFOp3m61//Otu3b6e6upozzjiDiooKFi9enKUWiojs2egK+D9eB9ve7Dd5SleKSMQgFh38Mg87Hs77zl6f7j1c8OOPP869997LK6+8gnOOCy+8kGeffZba2lomTJjAww8/DPgxaoqLi/nBD37A4sWLqaioGHy7REQOkEo0g/D444/z+OOPM2fOHE488UTWrFnDunXrOP7443niiSf4yle+wnPPPUdxcfFwN1VEZJT14PfS0968tYmCnBiTy7JyT+8ezjmuv/56rr766n7PLVu2jEceeYSvfe1rnHXWWXzjG9/IaltERN5LOHrwWbxOsvdwweeeey633XYbLS0tAGzZsoWamhqqq6vJz8/niiuu4Nprr2XZsmX9XisiMtRGVw9+H4ZiuODzzjuPyy+/nIULFwJQWFjI7bffzvr167n22muJRCLE43FuuukmAK666ioWLVrEhAkTdJJVRIZcKIYLXrutibxEjMOzXKLJNg0XLCKDFfrhgtFwwSIi/YQk4EEJLyKyu1ER8AMpI432eB9JpTIRCYcRH/C5ubnU19eHOgCdc9TX15ObmzvcTRGREBnxV9FMmjSJqqoqamtr9zrP9qYOYtEIbTWJIWzZwZWbm8ukSZOGuxkiEiIjPuDj8TjTpk3b5zzX/PAZplcWctMVs4aoVSIiI9+IL9EMhGFkQlzCERHZH1ntwZvZJqAZSAOpvV2reeDrAeW7iMjuhqJEc4Zzri7bK1G+i4jsLhwlGjP14EVE+sh2wDvgcTNbamZX7WkGM7vKzJaY2ZJ9XSmzL9azKhER6ZbtgD/NOXcicB7wBTP7i74zOOduds7Nc87Nq6ys3K+VqAYvItJfVgPeObcl+F4DPADMz8Z6Imbqv4uI9JG1gDezAjMr6v4ZOAdYkZ11ocskRUT6yOZVNOOAB8ysez3/65x7NBsrMlSiERHpK2sB75zbCAzNR0tVohER6Sccl0mi0RhFRPoKR8Bn8Z6sIiKjVTgCHtXgRUT6CkXA+8sklfAiIr2FIuDNIJMZ7laIiIws4Qh41IMXEekrFAGPhioQEeknFAFvaKgxEZG+whHwSngRkX7CEfCqwYuI9BOKgI9EVIMXEekrFAGvm26LiPQXjoA3leBFRPoKRcCDSjQiIn2FIuBNwwWLiPQTjoAHdeFFRPoIRcBHVIMXEeknFAFvpqtoRET6CkfAowqNiEhf4Qh4DTYmItJPKAIedBWNiEhfoQh434NXxIuI9BaOgB/uBoiIjEChCPiImWrwIiJ9ZD3gzSxqZq+Z2UPZWwe6TFJEpI+h6MFfA6zO5go02JiISH9ZDXgzmwRcAPwiq+vBdJJVRKSPbPfgfwT8E5DZ2wxmdpWZLTGzJbW1tfu3FvXgRUT6yVrAm9mHgBrn3NJ9zeecu9k5N885N6+ysnK/1qWTrCIi/WWzB38qcKGZbQJ+C5xpZrdnY0URXQcvItJP1gLeOXe9c26Sc24q8AngT865K7KxrogZaQW8iMhuQnMdfGavVX4RkUNTbChW4px7Gng6W8tXiUZEpL/w9OCV7yIiuwlHwEdQDV5EpI9wBLzpg04iIn2FJuBVohER2V1IAh7SSngRkd2EI+Ajuum2iEhf4Qh4DVUgItJPSAJe48GLiPQVkoA31eBFRPoIR8BHVKIREekrHAGvEo2ISD8hCXiNJiki0ldoAt45DTgmItJbaAIeUB1eRKSXkAS8/64yjYjILuEI+CDhdaJVRGSXcAS8SjQiIv2EJOD9d/XgRUR2CUnA+4TXp1lFRHYJR8D31OCHuSEiIiPIgALezK4xszHm3Wpmy8zsnGw3bqC6SzS6Dl5EZJeB9uD/2jnXBJwDlAJXAt/JWqsGSSUaEZH+BhrwQR+Z84HfOOdW9po27HadZB3edoiIjCQDDfilZvY4PuAfM7MiIJO9Zg1Odw1eJRoRkV1iA5zvs8BsYKNzrs3MyoDP7OsFZpYLPAvkBOu51zn3zQNp7N50l2jUgxcR2WWgPfiFwFrnXIOZXQF8DWh8j9d0Amc652bh3xwWmdmC/W/q3mmoAhGR/gYa8DcBbWY2C/gysAH49b5e4LyW4GE8+MpKAvf04NWFFxHpMdCATzlf4P4I8N/OuRuBovd6kZlFzWw5UAM84Zx7eQ/zXGVmS8xsSW1t7WDa3kNDFYiI9DfQgG82s+vxl0c+bGYRfI98n5xzaefcbGASMN/MjtvDPDc75+Y55+ZVVlYOpu09IsFWqEQjIrLLQAP+4/ia+l8757bhA/s/B7oS51wDsBhYNOgWDoCugxcR6W9AAR+E+h1AsZl9COhwzu2zBm9mlWZWEvycB5wNrDnA9u5RLOjCa7AxEZFdBjpUwaXAK8DHgEuBl83skvd42XhgsZm9AbyKr8E/dCCN3ZtocBlNMj1iLs0XERl2A70O/qvASc65GvC9c+BJ4N69vcA59wYw54BbOADxqEo0IiJ9DbQGH+kO90D9IF6bdd09+JQCXkSkx0B78I+a2WPAncHjjwOPZKdJg9ddg0+lFfAiIt0GFPDOuWvN7GLg1GDSzc65B7LXrMGJRbt78KrBi4h0G2gPHufcfcB9WWzLfotFVIMXEelrnwFvZs3seXgBw49GMCYrrRqknhq8SjQiIj32GfDOufccjmAkiEeDGrx68CIiPUbMlTAHItpTolENXkSkWygCPtbzQSf14EVEuoUj4IMSjU6yiojsEo6A1wedRET6CUXA77qKRjV4EZFuoQj4XR90Ug9eRKRbOAI+ohq8iEhfoQh4DRcsItJfKAJewwWLiPQXioDXcMEiIv2FIuA1XLCISH+hCPhoxDDTUAUiIr2FIuDBf9hJJRoRkV1CE/BRBbyIyG5CE/DxSEQ1eBGRXkIT8NGo6ZZ9IiK9hCbgVYMXEdldiAI+QlolGhGRHqEJ+GjESKpEIyLSI2sBb2aTzWyxma0ys5Vmdk221gV+REkNVSAisks2e/Ap4MvOuZnAAuALZjYzK2tafifHunWqwYuI9JK1gHfObXXOLQt+bgZWAxOzsrKH/4EzUy/ohh8iIr0MSQ3ezKYCc4CX9/DcVWa2xMyW1NbW7t8KonHillaJRkSkl6wHvJkVAvcBX3LONfV93jl3s3NunnNuXmVl5f6tJBInYWmVaEREeslqwJtZHB/udzjn7s/aiqJx4qT0SVYRkV6yeRWNAbcCq51zP8jWeoAg4NP6JKuISC/Z7MGfClwJnGlmy4Ov87OypoivwSfVgxcR6RHL1oKdc88Dlq3l7yaaIGEpOlPpIVmdiMhoEI5PskZjJCxNR1IlGhGRbiEJ+AQJ0nQk1YMXEekWjoCPxImpBy8isptwBHxwmWSnevAiIj1CFfAdOskqItIjJAGfIOZSJNNO49GIiATCEfCRGFF8770jpYAXEYGwBHwsh4TrAqCpPTnMjRERGRnCEfB5peSk/Dhmtc2dw9wYEZGRIRwBn19OvKuRKGkFvIhIIDQBbziKaaW+VQEvIgJhCfi8MgBKrZlG1eBFRICwBHzuGADGRDoU8CIigXAEfE4RAOMSXQp4EZFAqAK+MtHFzjYFvIgIhCzgpxVlWFXd77avIiKHpJAEvK/BzyjO8HZdKy2dqWFukIjI8AtHwOcWA/D+Dd8nl0421bUOc4NERIZfOAI+Eu35cYLVs1EBLyISkoAH+MiNAIy1BvXgRUQIU8BPmg/AcYVNrN6qE60iIuEJ+PLpkFfKB/PWsWzzTpxzw90iEZFhFZ6Aj0Rh/CxObnyUmqZ2tjS0D3eLRESGVXgCHiCdwnD8ReRNlm1uGO7WiIgMq6wFvJndZmY1ZrYiW+vo5wPXATAzp47fv7ZlyFYrIjISZbMH/0tgURaX39+UUyGawzllW3lqTQ1vVKkXLyKHrqwFvHPuWWBHtpa/R5EIzPwIs3Y+SQnN/O616iFdvYjISDLsNXgzu8rMlpjZktra2gNf4GlfIpLp4ktjX+ehN6rpSKYPfJkiIqPQsAe8c+5m59w859y8ysrKA1/guGMB+HTTT0k213L1b5Ye+DJFREahYQ/4rJh1GQD/kn8vz7xVy9J3dg5zg0REhl44A/7C/wLgI5knuTb393z6tlfY2qjr4kXk0JLNyyTvBF4EjjazKjP7bLbW1U80Dn/zFABf4C5OS73IP9//JpmMPt0qIoeObF5Fc5lzbrxzLu6cm+ScuzVb69qjSfNg0X8AcFP8h1Suv5ujvvZH1te0DGkzRESGSzhLNN0WfB4uvweA78Zv4Y7YDXz110+QTOnKGhEJv3AHPMBR58B53wXg5Mga7mr5NPFvlfHCG2uHuWEiItkV/oAHOPlq+PJbu01quOdv+favf8/2Vc9Dp8o2IhI+NpKG1Z03b55bsmRJ9laQ6oLlt5N55j+JNO/+KdfkX95K/PiLIZ0EM3+iVkRkhDOzpc65eXt87pAK+G6dzaTv+hTRjU/1e8olCrD8cjjqPDj+Eph0kg98EZERSAG/D82tbfzu5bUc+8IXOTH9Rv8ZSqfBtL+AmlVw1jf8zyIiI4QCfgBcJs2fX3qe555+jI913M/0yFY2ZMYzPbJ19xkLxkLl0f7TsnVv+ZJOyza44PuQVzosbReRg6h9J8TzIZaz7/nqN0DZEXs+wu9ogme/Cyd+2s8Tyd7pTgX8IDW2JXll0w5+8MRbvG/7w1wSfZZ6xnBh9MX3fvHcT8PEedC0BbYshfxyOOdb8PYzMP1M/ybQ/TtX6Udk/yU7oKMRisbte772nRBNQKLAn4fb+DSMmeBv87n2ERgzCdrqId0JMy+CG0qg4mg48iw48VOwcxO01sCcK6HhHXj0nyF3DLx+J8y6HLqaYc5fwfQzoKkaXroJti6Hzb3yYtrp/rxebjGUTIETr4Str8Mj/wSHHeev9KuYsV+/BgX8AdjR2sW3H1nNPUurAMckq+N420ihtfM3hS9SkdxKeaZu4AucfxUsv9P/USQK4ahFkOqAggo4+9/gd//HvzlEE3DKF+Goc/0fRiYDOzb6P0q9MchI0bYDYrmQyN/7PO07YdsKf7+G6mVQONZ3dOrW+VBb+QDUrvW33SwaDyd/Hlq2w9JfwbEXQcnhEM/zy9r2Jmx+CU64FG5cAM3V/mj63H/3r6leDi//zL+moMIv950X/GuLD4fGzdn/neyPWB5cux5yCgf9UgX8QXTXq5u5cfEGJpXm8ecN9T3TjQxHxWqYWZpmweR85th6JiY3UfDWA7teXDbdhzTB77x0Gux8+71XOmaiD32AaA7M+oTvuWx+CY78ILTV+bLRxHnw3PfguEvghI/7f6S2enjlFki2wrEf9T2TjkboaICGd2H8Cb43A/7IYuvr/p8vr8T3NnqrXeuX2VoPFUf6ackO/2aUxUPQIZFs92+0+yqzvdeRl3P9n3vhJzD1VEgU+cDJLfZB1tuq38OS2+Cy3/oga6r27egOtUwm2P/O7/+icb402NUC1a/50Jt/ld8PGGRS8PS3/bpcGsbOhE3P++l5pTD9LP9J71W/852NKQvhmAv9/Dvf8X9DxZNhzHjY/LJ/zQe/Cbkl8NCXYNWD8NGfwd1X7tqGC/8bOpvh3Zf89sy6DFwGInFYfvsefllGz/9BX7Fcvy+GythjoWblwVnWMR+G5u1Q9QrkFENnI0x9P3z4x36fvfWov3DjiW/AliDrTvsHmDgXjvnQfq1SAZ8lHck0tzy7kcPL87nzlc28tHHP9zeZPWkMU/PaOe6oIzks3sYpM6dSmJ9PPGpYzSpIdcJbj/lDwUS+H/J41YO+rDOc4vk+FPLLYfJ8/4/b2+QF/h962un+8Ldluw/8q5+BtX+Eh/4e5n/Ov/m8ea9fzil/B1uW+e2sOMr/MxdPhue+D6sfhBlnAwYv/xwuvwuSbf51v7nIHw6vfhDGz4Y5V8Crv4CWGn9oPH6WD7GNi/2bW16pfzMtnuTDpmmLn7d2DUyaDxaBWAIaq/zv/75gqKTJJ/te4IQ5vrS2/A5fQ33lZmh8189zwff9ZyfWPuLnq3vLh1vVq/4cTWeT/ydub4Dtb/b/vZYdATljYMY5sP5J36sdjFmX+fLAgZg415cQR4p4vt/Xg3Xpb3Z/ozl8of9a8zDUrYXDToALfwIv3ggFlTDvs/DO8/CHa/w6P3kPTD3NH4l8d5qf5/K7YeKJsGExLPsVXHybX1Yk5o9Gxs/yb17xPLjn03D0BTDjg/6IpPuqu0y6/xt5lijgh1Am42juTPHQG9V89QF/O9pELEJXKrPH+c+eOY75U8uoLMohPxFl1uQSxo3J3cOC07t6kDs2+MDMK4VNz/ne19pH4PAFcPzHYOXv4PX/hQkn+iOEDX/yr+s+N9C8FQrH+UAeCSzi/2Fk8KI5vna8J2f/m683173lS4Gv3uKnTzvd9/rbgw7JCR+HNY/4smEk7t+AJs71b47xPHjqBj9ffrk/Iuz+25n5EX808Kdv+Rr1mV/3ZcfSKVCz2pdJLAL5ZT781jwMp34RymfA67/15ZKFf+uvUJt5kW9r0XgfpIVj4bXbYcJsKJ0K+RWA23VUsPR/fG+56LBd27unI6jB2PG23+ZR9hkYBfwwc87xTn0b62paWP7uTpa/28AL6315Jz8Rpa1r97FxygsSNHek6EpnOO3ICi6ZO4nceJSCnCgzx4+hamc70yoLGJN7EP4QO1v8CaHyGf4P2yL+n7Sj0ZddXvixn2/6Wf7IIr/M/yPueNsf/m9YDGsegvO/51//h2t8L6h4ku89te8Mejbz/JtUy3Yom+ZPOL95L6S7fHBsX+l719NO9yWn/ArIKYK3/ujLUCvu86WG0qk+sMbP8uWDpirfvjlX+Offftb/0xeNh6e/49dTfqQ/OVZ2hD+n8dJNfn2N7/pe9FGLfMnqqX/1y5o03x9d7NgAx13sgyOn0F8i21Lja7wFlX65hx3vSyrRuF/Wm/f6N9QpCyFe4LcB59+Mxx0HT/6LL4Xkl/ltTxT6o5a2en/0s/4JfzKvbp1f3tvP+Np13Vr/O091+vktAvXr/Jv4izf6eVu2+WCufs1/7y7xdEunfK/98JP94y1L/TZ0l+Jcd4AOTc9TDg4F/AiWyTg21rXQlXLsbOvil3/exKa6Vta9x6iXObEI44tzOWpcEc0dKRYcUc7MCWM47cgKutIZ8uJRErEIzR1Jig7GG8FIlery9drcMQe+rK62fZ8sFBmBFPCjkHOO9mSa3FiUpZt3snZbMw1tXTS0JalubOfptbX9ev69mUFhTozmjhS58QhXnDyFrY0dHDm2kItPnERXOsPEkjxWbW2isjCHw8sVbCKjkQI+hDIZRyRirNveTFVDOxNL8nizqpGNdS1kHLR1pljyzk5WVjcNaHn5iSgRMyaW5JGIRVg4vZzlmxv48OwJnHh4CQWJGBEzJpfl0ZnKkIhGiER0uabIcNtXwMeGujFycHSH64xxRcwYVwTAUcH3vtq70pjBE6u209qZoq0rTUN7krbOFK1dKTqTGRrbkzR3pHhlkz/x9uaWRoCex3uSiEaYWJrH6UdVUl6QoLalk9L8BNMqCthY20JNcyefP306rV0pZo4fg+n6fZEhpR687MY5x6ubdjK+OJfNO9p4p76NJ1ZtIy8RJRGN8HpVI2/Xte738otyY3SmMnSlMpj583pzp5Ry1LhCygtyKMqNMXPCGJLpDFPLC0imHTPGFtLQnqQkL+6vQHMQ1dGDCKASjRxkzrndeuM7WrvYVN/K9MpCnnmrlsKcKO1dGR5ftY1ENMKm+lbak2lWbBlYuWigFhxRxomHl9KVytDUkWRTXRsnH1HGwunlrNzSRDKT4ePzJpNMO0ry47R2pijIieEc5MYjOqKQUFDAy4iSSmfY3tzJuKIcWjpTNHekWP5uAyurm2juSFKQE+PYCWP46gMraOlMMak0j6qd7VlpyxGVBcwYW8hjK7dTkIjyf884ktL8BI+u3EbUYEJJHmccPZb5R5Tx7o42ygtySKYzpDMOM5hcmk9HKk064yjKjZPOOB1dyJBSwEsoVDe0U1aQYEtDO4U5/qRvPGosXltDdUMH0yoKqG/ppLqxg6fX1rKxtoVFxx3GxtpWGtuTTK8s4KWNO2hP7rr66LAxudQ0d5A5wH+D7nJTb0W5MWZPLmHd9ha2NfmP3l99+hFMryyksiiH59fVMW9KKRVFOXQmMzgcxXlxcuNRWjpTrN7axCVzJ1Hb3ElFYQ658WjP0VNTR5KinFiwbr2hHMoU8CK9pDOOzlSamqZOplYUsLO1i45U2r957GxnQ20r62qaqWvuYlpFPo3tSW5cvIH2ZJr3z6igtTPFlPIC0hnHW9ub2VDbgmF0pft/Gjc3HqEjeXA+pTuhOJe61i4KElEa2pMcXpZPfUsXLZ0pjhpXSE4syvamDo4+rIjKohzWbG2mqVnl+gkAAAo3SURBVCPJKdPLKcr1JaqZE8aQzjhK8xOcMr2cFzfWkxOLcOTYIlo6UxjQ3OGXF49GiEWNrlSGMXlxqna28+rbO7j0pMn92ta3bCdDRwEvMgSqG9oZX+yHmWjtShOLGNGIsXlHG0dUFNDQlmRLQzsbaltIpR3zp5WxYksj97+2hYjB1IoCinJirNraxPjiPFZsaaSmuZNN9a3MPbyUvESU2uZOmjtSlBUkWLuteY9vKkMhFjFSwWHPmNwYTR0pznrfWNbVtDCxJI+8RJSjDyvinfpWivMSvPx2Pcl0hk+cdDhPr62hotCPtV5emCAWifB6VQPzp5UxvaKQls4UK6ob+f3yas44eizXnXc0SzbtZPbhJZQX5FDX0klbV4ra5k7eP6OS9mSaisIcmjqS5MejRCNGQ1uSjXWtzJ1SSl1LJ+UFCcxstzei7uzrO320UcCLhFBbV4q8eJTtTZ2U5MdZu62Zw4pzKStIsLOti421rUwsyePtutaey2NbOlM0tSd5YUMdU8sLGFuUQ35OjDVbm2hPpknEory0sZ6oGc0dSQDKChNMKSugrqWTNduaMYNTppf3DLfRrSQ/TltXeq/jLg2VaMRIB28+48bksL3Jj9UzJjdGS2eKyqJd0yqLcqht7ux5jZm//HdGcATT0pEiLxFl3fYW5hxeQsSM59fXEYsYR44t5NxjD+PPG+owjNxElHlTSsmLR2lsT7KxroWF0ysw/CXKGecoyU+QH48yrjiXP7xezYSSXBLRCNee+z5mTti/T2MPW8Cb2SLgx0AU+IVz7jv7ml8BLzK6dJ9U7u4Bd38AD6C1M8XK6ibGF+dS3dDOEZWFrN3WzLTKAmIR44lV2xlfnEtePMqYvDjxaITNO9ooSESJxyK8WdXIzrYu1te0UFqQYPyYXDIOapo7eHZdLWOLcjlhUjGb6lrpTGX484Z6rj79CGJBD35LQzsdyXTPKK9lBQmOm1hMa2eKFVsaOXJsITmxCMs2N/QE/XDJiUVY82+L9usoYlg+6GRmUeBG4GygCnjVzB50zq3K1jpFZGh1XzHUHUy9P91ckBNj/rQyACaX+aEwKot23QbvigVT+i3v6MN2fVjvpKllB62dHck0ufF9D6LWu2RT3xKUxqaUUd/SSXFenNqWTvLjMepbO8mNR1m2eSdHjyuiqSNFOuMozIn1nCgvyU+wemsTK7c0knZQmh/npKll1DR30N6VoSQ/zpFjC1mzrZl/vOd15kwuoTOVec82DlbWevBmthD4F+fcucHj6wGcc9/e22vUgxcRGZx99eCzeRueicC7vR5XBdN2Y2ZXmdkSM1tSW1ubxeaIiBxahv0+a865m51z85xz8yorK4e7OSIioZHNgN8C9L5gdlIwTUREhkA2A/5VYIaZTTOzBPAJ4MEsrk9ERHrJ2lU0zrmUmf0t8Bj+MsnbnHMH6dblIiLyXrI6Hrxz7hHgkWyuQ0RE9mzYT7KKiEh2KOBFREJqRI1FY2a1wDv7+fIKoO4gNmc00DYfGrTN4Xcg2zvFObfHa8xHVMAfCDNbsrdPc4WVtvnQoG0Ov2xtr0o0IiIhpYAXEQmpMAX8zcPdgGGgbT40aJvDLyvbG5oavIiI7C5MPXgREelFAS8iElKjPuDNbJGZrTWz9WZ23XC352Axs8lmttjMVpnZSjO7JpheZmZPmNm64HtpMN3M7CfB7+ENMztxeLdg/5lZ1MxeM7OHgsfTzOzlYNvuCgavw8xygsfrg+enDme795eZlZjZvWa2xsxWm9nCsO9nM/v74O96hZndaWa5YdvPZnabmdWY2Ype0wa9X83sU8H868zsU4Npw6gO+F63BTwPmAlcZmYzh7dVB00K+LJzbiawAPhCsG3XAU8552YATwWPwf8OZgRfVwE3DX2TD5prgNW9Hv8H8EPn3JHATuCzwfTPAjuD6T8M5huNfgw86px7HzALv+2h3c9mNhH4IjDPOXccfjDCTxC+/fxLYFGfaYPar2ZWBnwTOBmYD3yz+01hQJxzo/YLWAg81uvx9cD1w92uLG3r7/H3t10LjA+mjQfWBj//HLis1/w9842mL/x9A54CzgQeAgz/Cb9Y332OH6l0YfBzLJjPhnsbBrm9xcDbfdsd5v3Mrru9lQX77SHg3DDuZ2AqsGJ/9ytwGfDzXtN3m++9vkZ1D54B3hZwtAsOSecALwPjnHNbg6e2AeOCn8Pyu/gR8E9AJnhcDjQ451LB497b1bPNwfONwfyjyTSgFvifoCz1CzMrIMT72Tm3BfgesBnYit9vSwn3fu422P16QPt7tAd86JlZIXAf8CXnXFPv55x/Sw/Nda5m9iGgxjm3dLjbMoRiwInATc65OUAruw7bgVDu51LgI/g3twlAAf1LGaE3FPt1tAd8qG8LaGZxfLjf4Zy7P5i83czGB8+PB2qC6WH4XZwKXGhmm4Df4ss0PwZKzKz73gW9t6tnm4Pni4H6oWzwQVAFVDnnXg4e34sP/DDv5w8Cbzvnap1zSeB+/L4P837uNtj9ekD7e7QHfGhvC2hmBtwKrHbO/aDXUw8C3WfSP4WvzXdP/6vgbPwCoLHXoeCo4Jy73jk3yTk3Fb8v/+Sc+ySwGLgkmK3vNnf/Li4J5h9VPV3n3DbgXTM7Oph0FrCKEO9nfGlmgZnlB3/n3dsc2v3cy2D362PAOWZWGhz5nBNMG5jhPglxEE5inA+8BWwAvjrc7TmI23Ua/vDtDWB58HU+vvb4FLAOeBIoC+Y3/BVFG4A38VcoDPt2HMD2fwB4KPj5COAVYD1wD5ATTM8NHq8Pnj9iuNu9n9s6G1gS7OvfAaVh38/ADcAaYAXwGyAnbPsZuBN/jiGJP1L77P7sV+Cvg21fD3xmMG3QUAUiIiE12ks0IiKyFwp4EZGQUsCLiISUAl5EJKQU8CIiIaWAFzkIzOwD3aNfiowUCngRkZBSwMshxcyuMLNXzGy5mf08GHu+xcx+GIxP/pSZVQbzzjazl4LxuR/oNXb3kWb2pJm9bmbLzGx6sPjCXuO63xF8SlNk2Cjg5ZBhZscAHwdOdc7NBtLAJ/GDXS1xzh0LPIMffxvg18BXnHMn4D9d2D39DuBG59ws4BT8pxXBj/j5Jfy9CY7Aj68iMmxi7z2LSGicBcwFXg0613n4wZ4ywF3BPLcD95tZMVDinHsmmP4r4B4zKwImOuceAHDOdQAEy3vFOVcVPF6OHwv8+exvlsieKeDlUGLAr5xz1+820ezrfebb3/E7Onv9nEb/XzLMVKKRQ8lTwCVmNhZ67o85Bf9/0D2K4eXA8865RmCnmb0/mH4l8IxzrhmoMrOLgmXkmFn+kG6FyACphyGHDOfcKjP7GvC4mUXwo/x9AX+TjfnBczX4Oj344Vx/FgT4RuAzwfQrgZ+b2b8Gy/jYEG6GyIBpNEk55JlZi3OucLjbIXKwqUQjIhJS6sGLiISUevAiIiGlgBcRCSkFvIhISCngRURCSgEvIhJS/x99pbxCoAvqWwAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(cnnhistory.history['loss'])\n",
        "plt.plot(cnnhistory.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A90k8W2B8bN7",
        "outputId": "7629e738-b42e-46d7-e203-06e3df9a49f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "from google.colab import drive\n",
            "drive.mount('/content/drive')\n",
            "!pip install librosa\n",
            "import librosa\n",
            "from librosa import display\n",
            "\n",
            "data, sampling_rate = librosa.load('/content/drive/MyDrive/Datasets/Audio_Speech_Actors_01-24/Actor_01/03-01-01-01-01-01-01.wav')\n",
            "% pylab inline\n",
            "import os\n",
            "import pandas as pd\n",
            "import glob \n",
            "\n",
            "plt.figure(figsize=(12, 4))\n",
            "librosa.display.waveplot(data, sr=sampling_rate)\n",
            "# Loading saved models\n",
            "\n",
            "X = joblib.load('/content/drive/MyDrive/Colab Notebooks/emotion-classification-from-audio-files/joblib_features/X.joblib')\n",
            "y = joblib.load('/content/drive/MyDrive/Colab Notebooks/emotion-classification-from-audio-files/joblib_features/y.joblib')\n",
            "# Loading saved models\n",
            "import joblib\n",
            "X = joblib.load('/content/drive/MyDrive/Colab Notebooks/emotion-classification-from-audio-files/joblib_features/X.joblib')\n",
            "y = joblib.load('/content/drive/MyDrive/Colab Notebooks/emotion-classification-from-audio-files/joblib_features/y.joblib')\n",
            "from sklearn.model_selection import train_test_split\n",
            "\n",
            "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
            "from sklearn.tree import DecisionTreeClassifier\n",
            "dtree = DecisionTreeClassifier()\n",
            "dtree.fit(X_train, y_train)\n",
            "predictions = dtree.predict(X_test)\n",
            "from sklearn.metrics import classification_report,confusion_matrix\n",
            "print(classification_report(y_test,predictions))\n",
            "from sklearn.ensemble import RandomForestClassifier\n",
            "rforest = RandomForestClassifier(criterion=\"gini\", max_depth=10, max_features=\"log2\", \n",
            "                                 max_leaf_nodes = 100, min_samples_leaf = 3, min_samples_split = 20, \n",
            "                                 n_estimators= 22000, random_state= 5)\n",
            "rforest.fit(X_train, y_train)\n",
            "x_traincnn = np.expand_dims(X_train, axis=2)\n",
            "x_testcnn = np.expand_dims(X_test, axis=2)\n",
            "x_traincnn.shape, x_testcnn.shape\n",
            "import keras\n",
            "import numpy as np\n",
            "import matplotlib.pyplot as plt\n",
            "import tensorflow as tf\n",
            "from keras.preprocessing import sequence\n",
            "from keras.models import Sequential\n",
            "from keras.layers import Dense, Embedding\n",
            "from keras.utils import to_categorical\n",
            "from keras.layers import Input, Flatten, Dropout, Activation\n",
            "from keras.layers import Conv1D, MaxPooling1D\n",
            "from keras.models import Model\n",
            "from keras.callbacks import ModelCheckpoint\n",
            "\n",
            "model = Sequential()\n",
            "\n",
            "model.add(Conv1D(128, 5,padding='same',\n",
            "                 input_shape=(40,1)))\n",
            "model.add(Activation('relu'))\n",
            "model.add(Dropout(0.1))\n",
            "model.add(MaxPooling1D(pool_size=(8)))\n",
            "model.add(Conv1D(128, 5,padding='same',))\n",
            "model.add(Activation('relu'))\n",
            "model.add(Dropout(0.1))\n",
            "model.add(Flatten())\n",
            "model.add(Dense(8))\n",
            "model.add(Activation('softmax'))\n",
            "opt = keras.optimizers.rmsprop(lr=0.00005, rho=0.9, epsilon=None, decay=0.0)\n",
            "import keras\n",
            "import numpy as np\n",
            "import matplotlib.pyplot as plt\n",
            "import tensorflow as tf\n",
            "from keras.preprocessing import sequence\n",
            "from keras.models import Sequential\n",
            "from keras.layers import Dense, Embedding\n",
            "from tensorflow.keras.utils import to_categorical\n",
            "from keras.layers import Input, Flatten, Dropout, Activation\n",
            "from keras.layers import Conv1D, MaxPooling1D\n",
            "from keras.models import Model\n",
            "from keras.callbacks import ModelCheckpoint\n",
            "\n",
            "model = Sequential()\n",
            "\n",
            "model.add(Conv1D(128, 5,padding='same',\n",
            "                 input_shape=(40,1)))\n",
            "model.add(Activation('relu'))\n",
            "model.add(Dropout(0.1))\n",
            "model.add(MaxPooling1D(pool_size=(8)))\n",
            "model.add(Conv1D(128, 5,padding='same',))\n",
            "model.add(Activation('relu'))\n",
            "model.add(Dropout(0.1))\n",
            "model.add(Flatten())\n",
            "model.add(Dense(8))\n",
            "model.add(Activation('softmax'))\n",
            "opt = keras.optimizers.rmsprop(lr=0.00005, rho=0.9, epsilon=None, decay=0.0)\n",
            "import keras\n",
            "import numpy as np\n",
            "import matplotlib.pyplot as plt\n",
            "import tensorflow as tf\n",
            "from keras.preprocessing import sequence\n",
            "from keras.models import Sequential\n",
            "from keras.layers import Dense, Embedding\n",
            "from tensorflow.keras import to_categorical\n",
            "from keras.layers import Input, Flatten, Dropout, Activation\n",
            "from keras.layers import Conv1D, MaxPooling1D\n",
            "from keras.models import Model\n",
            "from keras.callbacks import ModelCheckpoint\n",
            "\n",
            "model = Sequential()\n",
            "\n",
            "model.add(Conv1D(128, 5,padding='same',\n",
            "                 input_shape=(40,1)))\n",
            "model.add(Activation('relu'))\n",
            "model.add(Dropout(0.1))\n",
            "model.add(MaxPooling1D(pool_size=(8)))\n",
            "model.add(Conv1D(128, 5,padding='same',))\n",
            "model.add(Activation('relu'))\n",
            "model.add(Dropout(0.1))\n",
            "model.add(Flatten())\n",
            "model.add(Dense(8))\n",
            "model.add(Activation('softmax'))\n",
            "opt = keras.optimizers.rmsprop(lr=0.00005, rho=0.9, epsilon=None, decay=0.0)\n",
            "import keras\n",
            "import numpy as np\n",
            "import matplotlib.pyplot as plt\n",
            "import tensorflow as tf\n",
            "from keras.preprocessing import sequence\n",
            "from keras.models import Sequential\n",
            "from keras.layers import Dense, Embedding\n",
            "from tensorflow.keras.utils import to_categorical\n",
            "from keras.layers import Input, Flatten, Dropout, Activation\n",
            "from keras.layers import Conv1D, MaxPooling1D\n",
            "from keras.models import Model\n",
            "from keras.callbacks import ModelCheckpoint\n",
            "\n",
            "model = Sequential()\n",
            "\n",
            "model.add(Conv1D(128, 5,padding='same',\n",
            "                 input_shape=(40,1)))\n",
            "model.add(Activation('relu'))\n",
            "model.add(Dropout(0.1))\n",
            "model.add(MaxPooling1D(pool_size=(8)))\n",
            "model.add(Conv1D(128, 5,padding='same',))\n",
            "model.add(Activation('relu'))\n",
            "model.add(Dropout(0.1))\n",
            "model.add(Flatten())\n",
            "model.add(Dense(8))\n",
            "model.add(Activation('softmax'))\n",
            "opt = keras.optimizers.rmsprop(lr=0.00005, rho=0.9, epsilon=None, decay=0.0)\n",
            "import keras\n",
            "import numpy as np\n",
            "import matplotlib.pyplot as plt\n",
            "import tensorflow as tf\n",
            "from keras.preprocessing import sequence\n",
            "from keras.models import Sequential\n",
            "from keras.layers import Dense, Embedding\n",
            "from tensorflow.keras.utils import to_categorical\n",
            "from keras.layers import Input, Flatten, Dropout, Activation\n",
            "from keras.layers import Conv1D, MaxPooling1D\n",
            "from keras.models import Model\n",
            "from keras.callbacks import ModelCheckpoint\n",
            "from tensorflow.keras import optimizers\n",
            "\n",
            "model = Sequential()\n",
            "\n",
            "model.add(Conv1D(128, 5,padding='same',\n",
            "                 input_shape=(40,1)))\n",
            "model.add(Activation('relu'))\n",
            "model.add(Dropout(0.1))\n",
            "model.add(MaxPooling1D(pool_size=(8)))\n",
            "model.add(Conv1D(128, 5,padding='same',))\n",
            "model.add(Activation('relu'))\n",
            "model.add(Dropout(0.1))\n",
            "model.add(Flatten())\n",
            "model.add(Dense(8))\n",
            "model.add(Activation('softmax'))\n",
            "opt = keras.optimizers.rmsprop(lr=0.00005, rho=0.9, epsilon=None, decay=0.0)\n",
            "import keras\n",
            "import numpy as np\n",
            "import matplotlib.pyplot as plt\n",
            "import tensorflow as tf\n",
            "from keras.preprocessing import sequence\n",
            "from keras.models import Sequential\n",
            "from keras.layers import Dense, Embedding\n",
            "from tensorflow.keras.utils import to_categorical\n",
            "from keras.layers import Input, Flatten, Dropout, Activation\n",
            "from keras.layers import Conv1D, MaxPooling1D\n",
            "from keras.models import Model\n",
            "from keras.callbacks import ModelCheckpoint\n",
            "from tensorflow.keras import optimizers\n",
            "\n",
            "model = Sequential()\n",
            "\n",
            "model.add(Conv1D(128, 5,padding='same',\n",
            "                 input_shape=(40,1)))\n",
            "model.add(Activation('relu'))\n",
            "model.add(Dropout(0.1))\n",
            "model.add(MaxPooling1D(pool_size=(8)))\n",
            "model.add(Conv1D(128, 5,padding='same',))\n",
            "model.add(Activation('relu'))\n",
            "model.add(Dropout(0.1))\n",
            "model.add(Flatten())\n",
            "model.add(Dense(8))\n",
            "model.add(Activation('softmax'))\n",
            "opt = optimizers.rmsprop(lr=0.00005, rho=0.9, epsilon=None, decay=0.0)\n",
            "import keras\n",
            "import numpy as np\n",
            "import matplotlib.pyplot as plt\n",
            "import tensorflow as tf\n",
            "from keras.preprocessing import sequence\n",
            "from keras.models import Sequential\n",
            "from keras.layers import Dense, Embedding\n",
            "from tensorflow.keras.utils import to_categorical\n",
            "from keras.layers import Input, Flatten, Dropout, Activation\n",
            "from keras.layers import Conv1D, MaxPooling1D\n",
            "from keras.models import Model\n",
            "from keras.callbacks import ModelCheckpoint\n",
            "from tensorflow.keras.optimizers import RMSprop as rm\n",
            "\n",
            "model = Sequential()\n",
            "\n",
            "model.add(Conv1D(128, 5,padding='same',\n",
            "                 input_shape=(40,1)))\n",
            "model.add(Activation('relu'))\n",
            "model.add(Dropout(0.1))\n",
            "model.add(MaxPooling1D(pool_size=(8)))\n",
            "model.add(Conv1D(128, 5,padding='same',))\n",
            "model.add(Activation('relu'))\n",
            "model.add(Dropout(0.1))\n",
            "model.add(Flatten())\n",
            "model.add(Dense(8))\n",
            "model.add(Activation('softmax'))\n",
            "opt = rm(lr=0.00005, rho=0.9, epsilon=None, decay=0.0)\n",
            "import keras\n",
            "import numpy as np\n",
            "import matplotlib.pyplot as plt\n",
            "import tensorflow as tf\n",
            "from keras.preprocessing import sequence\n",
            "from keras.models import Sequential\n",
            "from keras.layers import Dense, Embedding\n",
            "from tensorflow.keras.utils import to_categorical\n",
            "from keras.layers import Input, Flatten, Dropout, Activation\n",
            "from keras.layers import Conv1D, MaxPooling1D\n",
            "from keras.models import Model\n",
            "from keras.callbacks import ModelCheckpoint\n",
            "from tensorflow.keras.optimizers import RMSprop \n",
            "\n",
            "model = Sequential()\n",
            "\n",
            "model.add(Conv1D(128, 5,padding='same',\n",
            "                 input_shape=(40,1)))\n",
            "model.add(Activation('relu'))\n",
            "model.add(Dropout(0.1))\n",
            "model.add(MaxPooling1D(pool_size=(8)))\n",
            "model.add(Conv1D(128, 5,padding='same',))\n",
            "model.add(Activation('relu'))\n",
            "model.add(Dropout(0.1))\n",
            "model.add(Flatten())\n",
            "model.add(Dense(8))\n",
            "model.add(Activation('softmax'))\n",
            "opt = RMSprop(lr=0.00005, rho=0.9, epsilon=None, decay=0.0)\n",
            "model.summary()\n",
            "model.compile(loss='sparse_categorical_crossentropy',\n",
            "              optimizer=opt,\n",
            "              metrics=['accuracy'])\n",
            "cnnhistory=model.fit(x_traincnn, y_train, batch_size=16, epochs=1000, validation_data=(x_testcnn, y_test))\n",
            "plt.plot(cnnhistory.history['loss'])\n",
            "plt.plot(cnnhistory.history['val_loss'])\n",
            "plt.title('model loss')\n",
            "plt.ylabel('loss')\n",
            "plt.xlabel('epoch')\n",
            "plt.legend(['train', 'test'], loc='upper left')\n",
            "plt.show()\n",
            "plt.plot(cnnhistory.history['acc'])\n",
            "plt.plot(cnnhistory.history['val_acc'])\n",
            "plt.title('model accuracy')\n",
            "plt.ylabel('acc')\n",
            "plt.xlabel('epoch')\n",
            "plt.legend(['train', 'test'], loc='upper left')\n",
            "plt.show()\n",
            "%history\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vf1W7LgP2DA5"
      },
      "source": [
        "\n",
        "\n",
        "And now let's plot the accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "8yyFBt7ASPUe",
        "outputId": "eb63ea30-fcc0-4e5b-e29b-0c0267c440b3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(cnnhistory.history['accuracy'])\n",
        "plt.plot(cnnhistory.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('acc')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaZONl1mD8XD"
      },
      "source": [
        "Let's now create a classification report to review the f1-score of the model per class.\n",
        "To do so, we have to:\n",
        "- Create a variable predictions that will contain the model.predict_classes outcome\n",
        "- Convert our y_test (array of strings with our classes) to an array of int called new_Ytest, otherwise it will not be comparable to the predictions by the classification report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EO25uIL-9vqx"
      },
      "outputs": [],
      "source": [
        "predictions=model.predict(x_testcnn) \n",
        "classes_x=np.argmax(predictions,axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1i06grlBBSrn",
        "outputId": "20cd70f7-9c59-41e7-bae2-2ced000db298"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3, 3, 3, ..., 1, 6, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "classes_x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUHshx93CM_6",
        "outputId": "cd4865e0-24bd-4115-edbb-483330e17569"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3, 3, 3, ..., 1, 6, 2], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMxojpvWCxOs"
      },
      "outputs": [],
      "source": [
        "new_Ytest = y_test.astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W07EQaC8DE6i",
        "outputId": "1f441bd2-96be-46bb-9540-524e70530c2f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3, 3, 3, ..., 1, 6, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "new_Ytest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FW2XHdTtEedk"
      },
      "source": [
        "Okay, now we can display the classification report:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfVSRmMu96rC",
        "outputId": "245b9021-1c4f-4a87-ba3d-c9dd6c93528b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.85      0.90       192\n",
            "           1       0.71      0.89      0.79       123\n",
            "           2       0.88      0.82      0.85       264\n",
            "           3       0.81      0.84      0.82       275\n",
            "           4       0.87      0.85      0.86       252\n",
            "           5       0.81      0.87      0.84       241\n",
            "           6       0.87      0.84      0.86       197\n",
            "           7       0.89      0.85      0.87       190\n",
            "\n",
            "    accuracy                           0.85      1734\n",
            "   macro avg       0.85      0.85      0.85      1734\n",
            "weighted avg       0.85      0.85      0.85      1734\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "report = classification_report(new_Ytest, classes_x)\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hu1S5IowfSDG"
      },
      "source": [
        "And now, the confusion matrix: it will show us the misclassified samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdy09SCEd7Cl",
        "outputId": "e271c2fb-7dda-4a44-c2fd-d070a686d5db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[164  11   0  13   1   1   1   1]\n",
            " [  2 109   5   4   2   1   0   0]\n",
            " [  1  18 217   3   9   9   4   3]\n",
            " [  3   7   6 230   3  20   3   3]\n",
            " [  1   2   9   7 214   8   7   4]\n",
            " [  0   1   3  20   3 209   3   2]\n",
            " [  0   3   3   4   7   7 166   7]\n",
            " [  1   2   5   4   6   3   7 162]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "matrix = confusion_matrix(new_Ytest, classes_x)\n",
        "print (matrix)\n",
        "\n",
        "# 0 = neutral, 1 = calm, 2 = happy, 3 = sad, 4 = angry, 5 = fearful, 6 = disgust, 7 = surprised"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_ySPOyHxkZ3"
      },
      "source": [
        "# Save the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5kRmoD-sdHj",
        "outputId": "e0249a48-7c71-457b-e22d-ea0c58b8e0c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved trained model at /content/drive/MyDrive/Colab Notebooks/emotion-classification-from-audio-files/our_test_files/Emotion_Voice_Detection_Model66.h5 \n"
          ]
        }
      ],
      "source": [
        "model_name = 'Emotion_Voice_Detection_Model66.h5'\n",
        "save_dir = '/content/drive/MyDrive/Colab Notebooks/emotion-classification-from-audio-files/our_test_files'\n",
        "# Save model and weights\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "model_path = os.path.join(save_dir, model_name)\n",
        "model.save(model_path)\n",
        "print('Saved trained model at %s ' % model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNUiznKNwUtJ"
      },
      "source": [
        "# Reloading the model to test it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4oAv6Kx8RBE",
        "outputId": "33b3b8fc-4708-42aa-809b-c93c56b186a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_10 (Conv1D)          (None, 40, 128)           768       \n",
            "                                                                 \n",
            " activation_15 (Activation)  (None, 40, 128)           0         \n",
            "                                                                 \n",
            " dropout_10 (Dropout)        (None, 40, 128)           0         \n",
            "                                                                 \n",
            " max_pooling1d_5 (MaxPooling  (None, 5, 128)           0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " conv1d_11 (Conv1D)          (None, 5, 128)            82048     \n",
            "                                                                 \n",
            " activation_16 (Activation)  (None, 5, 128)            0         \n",
            "                                                                 \n",
            " dropout_11 (Dropout)        (None, 5, 128)            0         \n",
            "                                                                 \n",
            " flatten_5 (Flatten)         (None, 640)               0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 8)                 5128      \n",
            "                                                                 \n",
            " activation_17 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 87,944\n",
            "Trainable params: 87,944\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "loaded_model = keras.models.load_model('/content/drive/MyDrive/Colab Notebooks/emotion-classification-from-audio-files/our_test_files/Emotion_Voice_Detection_Model66.h5')\n",
        "loaded_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHtPzc0Y8hfZ"
      },
      "source": [
        "# Checking the accuracy of the loaded model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUi-Zjuf8hDB",
        "outputId": "bac52b17-7a00-4192-9748-ea2b20354552"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "55/55 [==============================] - 0s 2ms/step - loss: 0.5627 - accuracy: 0.8483\n",
            "Restored model, accuracy: 84.83%\n"
          ]
        }
      ],
      "source": [
        "loss, acc = loaded_model.evaluate(x_testcnn, y_test)\n",
        "print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pXH3y7S9A1N"
      },
      "source": [
        "# Thank you for your attention! To be continued.."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "EmotionsRecognition.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}